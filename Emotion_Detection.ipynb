{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf4e51c-4f01-483e-81ab-79825020ebc4",
   "metadata": {},
   "source": [
    "# Multi Modal Analysis Integrated Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a056e72-3c2d-4ae7-96f4-23cfb218a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all warnings\n",
    "\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        out = self.dropout(context)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Load models\n",
    "face_emotion_model = tf.keras.models.load_model(r'C:/Users/reeva/Desktop/690/best_model.keras')\n",
    "sleep_model = joblib.load(r'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/multi_label_model.joblib')\n",
    "\n",
    "# Load LSTM model with matching architecture\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "lstm_model = EmotionLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "checkpoint = torch.load(r'C:/Users/reeva/Desktop/690/best_emotion_model.pth', map_location=torch.device('cpu'))\n",
    "lstm_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "lstm_model.eval()\n",
    "\n",
    "# Create and fit a scaler with simulated data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], [0.5, 0.4, 0.3, 0.2, 0.1, 0.0]])\n",
    "\n",
    "\n",
    "# Emotion mappings\n",
    "emotion_mappings = {\n",
    "    \"face\": {'Happy': 'Happy', 'Sad': 'Stressed', 'Neutral': 'Bored', 'Angry': 'Stressed', 'Surprise': 'Happy'},\n",
    "    \"lstm\": {'Baseline': 'Bored', 'Stress': 'Stressed', 'Amusement': 'Happy', 'Meditation': 'Relaxed'},\n",
    "    \"sleep\": {'happiness': 'Happy', 'sadness': 'Sad', 'anger': 'Angry', 'surprise': 'Happy', 'fear': 'Stressed'}\n",
    "}\n",
    "\n",
    "common_emotions = ['Happy', 'Sad', 'Neutral', 'Angry', 'Relaxed', 'Stressed', 'Bored']\n",
    "\n",
    "\n",
    "# Map outputs to common emotions\n",
    "def map_emotion_values(module_values, mapping):\n",
    "    mapped_values = {emotion: 0.0 for emotion in common_emotions}\n",
    "    total_value = 0.0  # Track total for normalization\n",
    "\n",
    "    for emotion, value in module_values.items():\n",
    "        if emotion in mapping:\n",
    "            common_emotion = mapping[emotion]\n",
    "            scalar_value = float(value) if not isinstance(value, (list, np.ndarray)) else float(np.mean(value))\n",
    "            mapped_values[common_emotion] += scalar_value\n",
    "            total_value += scalar_value\n",
    "\n",
    "    if total_value > 0:\n",
    "        for emotion in mapped_values:\n",
    "            mapped_values[emotion] /= total_value\n",
    "\n",
    "    return mapped_values\n",
    "\n",
    "\n",
    "# Combine probabilities with weights\n",
    "def combine_emotion_values(mapped_values_list, weights):\n",
    "    combined_values = {emotion: 0.0 for emotion in common_emotions}\n",
    "    for mapped_values, weight in zip(mapped_values_list, weights):\n",
    "        for emotion, value in mapped_values.items():\n",
    "            combined_values[emotion] += value * weight\n",
    "    return combined_values\n",
    "\n",
    "\n",
    "# Face emotion detection\n",
    "def detect_face_emotion():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Webcam not accessible.\")\n",
    "        return {'Neutral': 1.0}\n",
    "\n",
    "    print('Initializing webcam...')\n",
    "    print(\"Press 'c' to capture a photo.\")\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Frame capture failed.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow('Press \"c\" to capture', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            roi_gray = cv2.resize(gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "            roi_rgb = np.expand_dims(np.stack([roi_gray] * 3, axis=-1) / 255.0, axis=0)\n",
    "            predictions = face_emotion_model.predict(roi_rgb)[0]\n",
    "            face_results = dict(zip(['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'], predictions))\n",
    "            #print(\"Face Detection Results:\", face_results)  # Debug print\n",
    "            return face_results\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return {'Neutral': 1.0}\n",
    "\n",
    "\n",
    "# LSTM emotion detection\n",
    "def detect_lstm_emotion(input_data):\n",
    "    tensor_data = torch.tensor(input_data, dtype=torch.float32).unsqueeze(0)\n",
    "    output = lstm_model(tensor_data)\n",
    "    probabilities = torch.softmax(output, dim=1).detach().numpy()[0]\n",
    "    lstm_results = dict(zip(['Baseline', 'Stress', 'Amusement', 'Meditation'], probabilities))\n",
    "    #print(\"LSTM Detection Results:\", lstm_results)  # Debug print\n",
    "    return lstm_results\n",
    "\n",
    "\n",
    "# Sleep emotion detection\n",
    "def detect_sleep_emotion(input_data):\n",
    "    input_array = np.array(input_data).reshape(1, -1)\n",
    "    input_normalized = scaler.transform(input_array)\n",
    "    predictions = sleep_model.predict_proba(input_normalized)[0]\n",
    "    sleep_results = dict(zip(['happiness', 'sadness', 'anger', 'surprise', 'fear'], predictions))\n",
    "    #print(\"Sleep Detection Results:\", sleep_results)  # Debug print\n",
    "    return sleep_results\n",
    "\n",
    "\n",
    "# Display results\n",
    "def display_results(face_results, lstm_results, sleep_results, combined, final_emotion):\n",
    "    #print(\"\\n--- Debugging: Raw Inputs to display_results ---\")\n",
    "    #print(\"Face Results:\", face_results)\n",
    "    #print(\"LSTM Results:\", lstm_results)\n",
    "    #print(\"Sleep Results:\", sleep_results)\n",
    "    #print(\"Combined Results:\", combined)\n",
    "    #print(\"Final Emotion:\", final_emotion)\n",
    "\n",
    "    print(\"\\n--- Emotion Detection Results ---\")\n",
    "    print(\"Face Emotion Results:\", face_results)\n",
    "    print(\"LSTM Emotion Results:\", lstm_results)\n",
    "    print(\"Sleep Emotion Results:\", sleep_results)\n",
    "    print(\"Combined Results:\", combined)\n",
    "    print(f\"Final Emotion: {final_emotion}\")\n",
    "\n",
    "\n",
    "# Main function\n",
    "def run_emotion_detection():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Face emotion detection\n",
    "    face_emotion = detect_face_emotion()\n",
    "    if not face_emotion:\n",
    "        print(\"Error: Face emotion detection failed!\")\n",
    "\n",
    "    # LSTM emotion detection\n",
    "    lstm_input = simpledialog.askstring(\n",
    "        \"Input\",\n",
    "        \"Provide physiological signals:\\n\"\n",
    "        \"1. ECG: Electrical activity of the heart.\\n\"\n",
    "        \"2. Resp: Breathing rate.\\n\"\n",
    "        \"Input Format: <ECG_signal>, <Resp_signal>\\n\"\n",
    "        \"Example: 0.85, 0.65\"\n",
    "    )\n",
    "    lstm_emotion = detect_lstm_emotion([list(map(float, lstm_input.split(',')))])\n",
    "    if not lstm_emotion:\n",
    "        print(\"Error: LSTM emotion detection failed!\")\n",
    "\n",
    "    # Sleep emotion detection\n",
    "    sleep_input = simpledialog.askstring(\n",
    "        \"Input\",\n",
    "        \"Provide six sleep metrics:\\n\"\n",
    "        \"1. prop_W: Wake.\\n\"\n",
    "        \"2. prop_1: NREM Stage 1.\\n\"\n",
    "        \"3. prop_2: NREM Stage 2.\\n\"\n",
    "        \"4. prop_3: NREM Stage 3.\\n\"\n",
    "        \"5. prop_4: NREM Stage 4.\\n\"\n",
    "        \"6. prop_R: REM Sleep.\\n\"\n",
    "        \"Input Format: <prop_W>, <prop_1>, <prop_2>, <prop_3>, <prop_4>, <prop_R>\\n\"\n",
    "        \"Example: 0.2, 0.1, 0.4, 0.2, 0.1, 0.5\"\n",
    "    )\n",
    "    sleep_emotion = detect_sleep_emotion(list(map(float, sleep_input.split(','))))\n",
    "    if not sleep_emotion:\n",
    "        print(\"Error: Sleep emotion detection failed!\")\n",
    "\n",
    "    # Map values\n",
    "    mapped_face = map_emotion_values(face_emotion, emotion_mappings['face'])\n",
    "    mapped_lstm = map_emotion_values(lstm_emotion, emotion_mappings['lstm'])\n",
    "    mapped_sleep = map_emotion_values(sleep_emotion, emotion_mappings['sleep'])\n",
    "\n",
    "    # Combine and determine final emotion\n",
    "    weights = [0.4, 0.3, 0.3]\n",
    "    combined = combine_emotion_values([mapped_face, mapped_lstm, mapped_sleep], weights)\n",
    "    final_emotion = max(combined, key=combined.get)\n",
    "\n",
    "    # Display results\n",
    "    display_results(mapped_face, mapped_lstm, mapped_sleep, combined, final_emotion)\n",
    "\n",
    "    return final_emotion  # Returning the final emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c246d09-56c0-4c48-a9aa-eb126a6cce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the emotion detection function\n",
    "detected_emotion = run_emotion_detection()\n",
    "print(f\"\\nDetected Emotion: {detected_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bbb7b-7313-4612-b4b2-5305987dff61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
