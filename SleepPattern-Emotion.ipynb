{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0708866c-bd41-4fea-925c-4b39de4f6453",
   "metadata": {},
   "source": [
    "# Sleep pattern Emotion Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b328a7-553c-4e30-bb33-49541e9d681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyedflib in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.38)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyedflib) (2.0.2)\n",
      "PSG File Signal Labels: ['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal', 'EMG submental', 'Temp rectal', 'Event marker']\n",
      "Sampling Frequency: 100.0\n",
      "File Duration: 79500.0\n",
      "Hypnogram Annotations (first 10): (array([    0., 30630., 30750., 31140., 31170., 31200., 31350., 31380.,\n",
      "       31440., 31500., 31530., 31650., 31680., 31800., 31830., 31890.,\n",
      "       31950., 32070., 32100., 32130., 32250., 32460., 32490., 32550.,\n",
      "       32670., 32700., 32850., 32910., 32940., 32970., 33000., 33120.,\n",
      "       33270., 33300., 33330., 33390., 33420., 33510., 35400., 35430.,\n",
      "       35640., 35700., 35790., 35940., 35970., 36840., 37020., 37260.,\n",
      "       37290., 37410., 37500., 37530., 38460., 38490., 38520., 38580.,\n",
      "       38610., 38640., 38670., 38700., 38730., 38760., 38910., 38970.,\n",
      "       39060., 39120., 39150., 39180., 39240., 39300., 39480., 39540.,\n",
      "       39570., 39600., 39870., 39900., 39960., 39990., 40200., 40230.,\n",
      "       40290., 40320., 40500., 41370., 41400., 41460., 41490., 41610.,\n",
      "       42480., 42510., 42540., 42570., 42870., 42900., 43290., 43320.,\n",
      "       43350., 43380., 43410., 43590., 43680., 43710., 43920., 43980.,\n",
      "       44010., 44040., 44220., 44280., 44340., 44400., 44520., 44640.,\n",
      "       44670., 44700., 44730., 44970., 45000., 45510., 45540., 45660.,\n",
      "       46140., 46230., 46260., 46380., 47070., 47130., 47160., 48150.,\n",
      "       48180., 48210., 48300., 48420., 48480., 48540., 48570., 48600.,\n",
      "       48630., 48660., 48780., 48990., 49020., 49140., 49200., 50190.,\n",
      "       50220., 50370., 50640., 51150., 51240., 51540., 51570., 52110.,\n",
      "       52260., 79500.]), array([3.063e+04, 1.200e+02, 3.900e+02, 3.000e+01, 3.000e+01, 1.500e+02,\n",
      "       3.000e+01, 6.000e+01, 6.000e+01, 3.000e+01, 1.200e+02, 3.000e+01,\n",
      "       1.200e+02, 3.000e+01, 6.000e+01, 6.000e+01, 1.200e+02, 3.000e+01,\n",
      "       3.000e+01, 1.200e+02, 2.100e+02, 3.000e+01, 6.000e+01, 1.200e+02,\n",
      "       3.000e+01, 1.500e+02, 6.000e+01, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       1.200e+02, 1.500e+02, 3.000e+01, 3.000e+01, 6.000e+01, 3.000e+01,\n",
      "       9.000e+01, 1.890e+03, 3.000e+01, 2.100e+02, 6.000e+01, 9.000e+01,\n",
      "       1.500e+02, 3.000e+01, 8.700e+02, 1.800e+02, 2.400e+02, 3.000e+01,\n",
      "       1.200e+02, 9.000e+01, 3.000e+01, 9.300e+02, 3.000e+01, 3.000e+01,\n",
      "       6.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       3.000e+01, 1.500e+02, 6.000e+01, 9.000e+01, 6.000e+01, 3.000e+01,\n",
      "       3.000e+01, 6.000e+01, 6.000e+01, 1.800e+02, 6.000e+01, 3.000e+01,\n",
      "       3.000e+01, 2.700e+02, 3.000e+01, 6.000e+01, 3.000e+01, 2.100e+02,\n",
      "       3.000e+01, 6.000e+01, 3.000e+01, 1.800e+02, 8.700e+02, 3.000e+01,\n",
      "       6.000e+01, 3.000e+01, 1.200e+02, 8.700e+02, 3.000e+01, 3.000e+01,\n",
      "       3.000e+01, 3.000e+02, 3.000e+01, 3.900e+02, 3.000e+01, 3.000e+01,\n",
      "       3.000e+01, 3.000e+01, 1.800e+02, 9.000e+01, 3.000e+01, 2.100e+02,\n",
      "       6.000e+01, 3.000e+01, 3.000e+01, 1.800e+02, 6.000e+01, 6.000e+01,\n",
      "       6.000e+01, 1.200e+02, 1.200e+02, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       2.400e+02, 3.000e+01, 5.100e+02, 3.000e+01, 1.200e+02, 4.800e+02,\n",
      "       9.000e+01, 3.000e+01, 1.200e+02, 6.900e+02, 6.000e+01, 3.000e+01,\n",
      "       9.900e+02, 3.000e+01, 3.000e+01, 9.000e+01, 1.200e+02, 6.000e+01,\n",
      "       6.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 1.200e+02,\n",
      "       2.100e+02, 3.000e+01, 1.200e+02, 6.000e+01, 9.900e+02, 3.000e+01,\n",
      "       1.500e+02, 2.700e+02, 5.100e+02, 9.000e+01, 3.000e+02, 3.000e+01,\n",
      "       5.400e+02, 1.500e+02, 2.724e+04, 6.900e+03]), array(['Sleep stage W', 'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage W', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 1', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage R', 'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage R', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage W', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage 2', 'Sleep stage R', 'Sleep stage 1',\n",
      "       'Sleep stage R', 'Sleep stage W', 'Sleep stage 1', 'Sleep stage W',\n",
      "       'Sleep stage 1', 'Sleep stage W', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage W', 'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 1', 'Sleep stage R', 'Sleep stage W',\n",
      "       'Sleep stage 1', 'Sleep stage R', 'Sleep stage W', 'Sleep stage 1',\n",
      "       'Sleep stage W', 'Sleep stage 1', 'Sleep stage W', 'Sleep stage 1',\n",
      "       'Sleep stage W', 'Sleep stage ?'], dtype='<U13'))\n"
     ]
    }
   ],
   "source": [
    "!pip install pyedflib\n",
    "\n",
    "import pyedflib\n",
    "import os\n",
    "\n",
    "base_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0'\n",
    "\n",
    "# Define paths\n",
    "psg_file_path = os.path.join(base_path, 'sleep-cassette', 'SC4001E0-PSG.edf')\n",
    "hypnogram_file_path = os.path.join(base_path, 'sleep-cassette', 'SC4001EC-Hypnogram.edf')\n",
    "\n",
    "# Load PSG file\n",
    "with pyedflib.EdfReader(psg_file_path) as psg_file:\n",
    "    n_signals = psg_file.signals_in_file\n",
    "    signal_labels = psg_file.getSignalLabels()\n",
    "    print(\"PSG File Signal Labels:\", signal_labels)\n",
    "    print(\"Sampling Frequency:\", psg_file.getSampleFrequency(0))\n",
    "    print(\"File Duration:\", psg_file.file_duration)\n",
    "\n",
    "# Load Hypnogram file\n",
    "with pyedflib.EdfReader(hypnogram_file_path) as hyp_file:\n",
    "    annotations = hyp_file.readAnnotations()\n",
    "    print(\"Hypnogram Annotations (first 10):\", annotations[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e6eae9-3e32-4979-a77f-426a77e7d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (3.9.2)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (2.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (24.1)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (1.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.5->mne) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->mne) (3.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->mne) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reeva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reeva\\AppData\\Local\\Temp\\ipykernel_8348\\1095414249.py:14: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  psg_data = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n",
      "C:\\Users\\reeva\\AppData\\Local\\Temp\\ipykernel_8348\\1095414249.py:14: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  psg_data = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n",
      "C:\\Users\\reeva\\AppData\\Local\\Temp\\ipykernel_8348\\1095414249.py:14: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  psg_data = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RawEDF | SC4001E0-PSG.edf, 7 x 7950000 (79500.0 s), ~424.6 MB, data loaded>\n",
      "PSG Channel Names: ['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal', 'EMG submental', 'Temp rectal', 'Event marker']\n",
      "Sampling Frequency: 100.0\n",
      "First 10 Annotations: (array([    0., 30630., 30750., 31140., 31170., 31200., 31350., 31380.,\n",
      "       31440., 31500., 31530., 31650., 31680., 31800., 31830., 31890.,\n",
      "       31950., 32070., 32100., 32130., 32250., 32460., 32490., 32550.,\n",
      "       32670., 32700., 32850., 32910., 32940., 32970., 33000., 33120.,\n",
      "       33270., 33300., 33330., 33390., 33420., 33510., 35400., 35430.,\n",
      "       35640., 35700., 35790., 35940., 35970., 36840., 37020., 37260.,\n",
      "       37290., 37410., 37500., 37530., 38460., 38490., 38520., 38580.,\n",
      "       38610., 38640., 38670., 38700., 38730., 38760., 38910., 38970.,\n",
      "       39060., 39120., 39150., 39180., 39240., 39300., 39480., 39540.,\n",
      "       39570., 39600., 39870., 39900., 39960., 39990., 40200., 40230.,\n",
      "       40290., 40320., 40500., 41370., 41400., 41460., 41490., 41610.,\n",
      "       42480., 42510., 42540., 42570., 42870., 42900., 43290., 43320.,\n",
      "       43350., 43380., 43410., 43590., 43680., 43710., 43920., 43980.,\n",
      "       44010., 44040., 44220., 44280., 44340., 44400., 44520., 44640.,\n",
      "       44670., 44700., 44730., 44970., 45000., 45510., 45540., 45660.,\n",
      "       46140., 46230., 46260., 46380., 47070., 47130., 47160., 48150.,\n",
      "       48180., 48210., 48300., 48420., 48480., 48540., 48570., 48600.,\n",
      "       48630., 48660., 48780., 48990., 49020., 49140., 49200., 50190.,\n",
      "       50220., 50370., 50640., 51150., 51240., 51540., 51570., 52110.,\n",
      "       52260., 79500.]), array([3.063e+04, 1.200e+02, 3.900e+02, 3.000e+01, 3.000e+01, 1.500e+02,\n",
      "       3.000e+01, 6.000e+01, 6.000e+01, 3.000e+01, 1.200e+02, 3.000e+01,\n",
      "       1.200e+02, 3.000e+01, 6.000e+01, 6.000e+01, 1.200e+02, 3.000e+01,\n",
      "       3.000e+01, 1.200e+02, 2.100e+02, 3.000e+01, 6.000e+01, 1.200e+02,\n",
      "       3.000e+01, 1.500e+02, 6.000e+01, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       1.200e+02, 1.500e+02, 3.000e+01, 3.000e+01, 6.000e+01, 3.000e+01,\n",
      "       9.000e+01, 1.890e+03, 3.000e+01, 2.100e+02, 6.000e+01, 9.000e+01,\n",
      "       1.500e+02, 3.000e+01, 8.700e+02, 1.800e+02, 2.400e+02, 3.000e+01,\n",
      "       1.200e+02, 9.000e+01, 3.000e+01, 9.300e+02, 3.000e+01, 3.000e+01,\n",
      "       6.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       3.000e+01, 1.500e+02, 6.000e+01, 9.000e+01, 6.000e+01, 3.000e+01,\n",
      "       3.000e+01, 6.000e+01, 6.000e+01, 1.800e+02, 6.000e+01, 3.000e+01,\n",
      "       3.000e+01, 2.700e+02, 3.000e+01, 6.000e+01, 3.000e+01, 2.100e+02,\n",
      "       3.000e+01, 6.000e+01, 3.000e+01, 1.800e+02, 8.700e+02, 3.000e+01,\n",
      "       6.000e+01, 3.000e+01, 1.200e+02, 8.700e+02, 3.000e+01, 3.000e+01,\n",
      "       3.000e+01, 3.000e+02, 3.000e+01, 3.900e+02, 3.000e+01, 3.000e+01,\n",
      "       3.000e+01, 3.000e+01, 1.800e+02, 9.000e+01, 3.000e+01, 2.100e+02,\n",
      "       6.000e+01, 3.000e+01, 3.000e+01, 1.800e+02, 6.000e+01, 6.000e+01,\n",
      "       6.000e+01, 1.200e+02, 1.200e+02, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       2.400e+02, 3.000e+01, 5.100e+02, 3.000e+01, 1.200e+02, 4.800e+02,\n",
      "       9.000e+01, 3.000e+01, 1.200e+02, 6.900e+02, 6.000e+01, 3.000e+01,\n",
      "       9.900e+02, 3.000e+01, 3.000e+01, 9.000e+01, 1.200e+02, 6.000e+01,\n",
      "       6.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 3.000e+01, 1.200e+02,\n",
      "       2.100e+02, 3.000e+01, 1.200e+02, 6.000e+01, 9.900e+02, 3.000e+01,\n",
      "       1.500e+02, 2.700e+02, 5.100e+02, 9.000e+01, 3.000e+02, 3.000e+01,\n",
      "       5.400e+02, 1.500e+02, 2.724e+04, 6.900e+03]), array(['Sleep stage W', 'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage W', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 1', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage R', 'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage R', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage W', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4',\n",
      "       'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2',\n",
      "       'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 4', 'Sleep stage 2', 'Sleep stage R', 'Sleep stage 1',\n",
      "       'Sleep stage R', 'Sleep stage W', 'Sleep stage 1', 'Sleep stage W',\n",
      "       'Sleep stage 1', 'Sleep stage W', 'Sleep stage 1', 'Sleep stage 2',\n",
      "       'Sleep stage W', 'Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 2', 'Sleep stage 3',\n",
      "       'Sleep stage 2', 'Sleep stage 1', 'Sleep stage R', 'Sleep stage W',\n",
      "       'Sleep stage 1', 'Sleep stage R', 'Sleep stage W', 'Sleep stage 1',\n",
      "       'Sleep stage W', 'Sleep stage 1', 'Sleep stage W', 'Sleep stage 1',\n",
      "       'Sleep stage W', 'Sleep stage ?'], dtype='<U13'))\n"
     ]
    }
   ],
   "source": [
    "!pip install mne\n",
    "\n",
    "import pyedflib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "\n",
    "# Paths to sample PSG and Hypnogram files\n",
    "psg_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/sleep-cassette/SC4001E0-PSG.edf'\n",
    "hypnogram_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/sleep-cassette/SC4001EC-Hypnogram.edf'\n",
    "\n",
    "# Load the PSG file using MNE for convenience\n",
    "psg_data = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n",
    "print(psg_data)\n",
    "sampling_rate = psg_data.info['sfreq']  # Sampling frequency\n",
    "channels = psg_data.ch_names  # Channel names\n",
    "\n",
    "print(\"PSG Channel Names:\", channels)\n",
    "print(\"Sampling Frequency:\", sampling_rate)\n",
    "\n",
    "# Load the Hypnogram file to get sleep stages\n",
    "with pyedflib.EdfReader(hypnogram_path) as hypnogram:\n",
    "    annotations = hypnogram.readAnnotations()\n",
    "\n",
    "# Display the first 10 annotations to understand the format\n",
    "print(\"First 10 Annotations:\", annotations[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c46e2e8-12a6-42c1-997f-2ea4e59a2670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep Stage Distribution Across All Files (in %):\n",
      "W: 60.70%\n",
      "1: 5.16%\n",
      "2: 18.11%\n",
      "3: 2.45%\n",
      "4: 1.43%\n",
      "R: 6.91%\n",
      "?: 5.25%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the dataset\n",
    "base_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0'\n",
    "\n",
    "# Dictionary to store cumulative time spent in each stage\n",
    "overall_stage_durations = Counter()\n",
    "\n",
    "# List all hypnogram files\n",
    "hypnogram_files = []\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if 'Hypnogram.edf' in file:\n",
    "            hypnogram_files.append(os.path.join(root, file))\n",
    "\n",
    "# Process each hypnogram file\n",
    "for hypnogram_path in hypnogram_files:\n",
    "    try:\n",
    "        with pyedflib.EdfReader(hypnogram_path) as hypnogram:\n",
    "            annotations = hypnogram.readAnnotations()\n",
    "\n",
    "            # Calculate time spent in each stage\n",
    "            stage_durations = Counter()\n",
    "            for onset, duration, annotation in zip(*annotations):\n",
    "                if \"Sleep stage\" in annotation:\n",
    "                    stage = annotation.split()[-1]\n",
    "                    stage_durations[stage] += duration\n",
    "\n",
    "            # Add to the overall stage durations\n",
    "            overall_stage_durations.update(stage_durations)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Could not read file {hypnogram_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert counts to percentages\n",
    "total_duration = sum(overall_stage_durations.values())\n",
    "stage_percentages = {stage: (duration / total_duration) * 100 for stage, duration in overall_stage_durations.items()}\n",
    "\n",
    "# Display the results\n",
    "print(\"Sleep Stage Distribution Across All Files (in %):\")\n",
    "for stage, percentage in stage_percentages.items():\n",
    "    print(f\"{stage}: {percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469272a7-0774-4ccf-aff1-f74e02e8b9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>prop_W</th>\n",
       "      <th>prop_1</th>\n",
       "      <th>prop_2</th>\n",
       "      <th>prop_3</th>\n",
       "      <th>prop_4</th>\n",
       "      <th>prop_R</th>\n",
       "      <th>prop_?</th>\n",
       "      <th>total_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SC4001EC-Hypnogram.edf</td>\n",
       "      <td>69.340278</td>\n",
       "      <td>2.013889</td>\n",
       "      <td>8.680556</td>\n",
       "      <td>3.506944</td>\n",
       "      <td>4.131944</td>\n",
       "      <td>4.340278</td>\n",
       "      <td>7.986111</td>\n",
       "      <td>86400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC4002EC-Hypnogram.edf</td>\n",
       "      <td>65.474123</td>\n",
       "      <td>2.049323</td>\n",
       "      <td>12.955887</td>\n",
       "      <td>3.265023</td>\n",
       "      <td>7.051059</td>\n",
       "      <td>7.467871</td>\n",
       "      <td>1.736714</td>\n",
       "      <td>86370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SC4011EH-Hypnogram.edf</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>3.784722</td>\n",
       "      <td>19.513889</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>5.902778</td>\n",
       "      <td>2.708333</td>\n",
       "      <td>86400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SC4012EC-Hypnogram.edf</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>3.194444</td>\n",
       "      <td>22.916667</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>86400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SC4021EH-Hypnogram.edf</td>\n",
       "      <td>66.215278</td>\n",
       "      <td>3.263889</td>\n",
       "      <td>18.923611</td>\n",
       "      <td>2.534722</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>5.659722</td>\n",
       "      <td>2.638889</td>\n",
       "      <td>86400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file_name     prop_W    prop_1     prop_2    prop_3    prop_4  \\\n",
       "0  SC4001EC-Hypnogram.edf  69.340278  2.013889   8.680556  3.506944  4.131944   \n",
       "1  SC4002EC-Hypnogram.edf  65.474123  2.049323  12.955887  3.265023  7.051059   \n",
       "2  SC4011EH-Hypnogram.edf  64.444444  3.784722  19.513889  3.333333  0.312500   \n",
       "3  SC4012EC-Hypnogram.edf  63.333333  3.194444  22.916667  2.777778  0.555556   \n",
       "4  SC4021EH-Hypnogram.edf  66.215278  3.263889  18.923611  2.534722  0.763889   \n",
       "\n",
       "     prop_R    prop_?  total_duration  \n",
       "0  4.340278  7.986111         86400.0  \n",
       "1  7.467871  1.736714         86370.0  \n",
       "2  5.902778  2.708333         86400.0  \n",
       "3  6.111111  1.111111         86400.0  \n",
       "4  5.659722  2.638889         86400.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store feature data for each file\n",
    "features_list = []\n",
    "\n",
    "# Loop through each hypnogram file and extract features\n",
    "for hypnogram_path in hypnogram_files:\n",
    "    try:\n",
    "        with pyedflib.EdfReader(hypnogram_path) as hypnogram:\n",
    "            annotations = hypnogram.readAnnotations()\n",
    "\n",
    "        # Calculate total time spent in each stage\n",
    "        stage_times = {'W': 0, '1': 0, '2': 0, '3': 0, '4': 0, 'R': 0, '?': 0}\n",
    "        total_time = 0\n",
    "\n",
    "        for onset, duration, annotation in zip(*annotations):\n",
    "            if \"Sleep stage\" in annotation:\n",
    "                stage = annotation.split()[-1]\n",
    "                if stage in stage_times:\n",
    "                    stage_times[stage] += duration\n",
    "                    total_time += duration\n",
    "\n",
    "        # Calculate the proportion of time spent in each stage\n",
    "        stage_proportions = {f\"prop_{stage}\": (time / total_time) * 100 for stage, time in stage_times.items()}\n",
    "        stage_proportions['total_duration'] = total_time  # Total duration of sleep in seconds\n",
    "\n",
    "        # Add file-specific features to the list\n",
    "        file_features = {'file_name': os.path.basename(hypnogram_path)}\n",
    "        file_features.update(stage_proportions)\n",
    "        features_list.append(file_features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {hypnogram_path}: {e}\")\n",
    "\n",
    "# Convert the list of features to a DataFrame for analysis\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "# Display the extracted features\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d44e58-cdee-4bdf-8627-111b05019748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unreadable files have been removed. A list of unreadable files is saved to C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/unreadable_files.txt.\n"
     ]
    }
   ],
   "source": [
    "import pyedflib\n",
    "import os\n",
    "\n",
    "# Define the base path and list for unreadable files\n",
    "base_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0'\n",
    "unreadable_files = []\n",
    "\n",
    "# Function to identify and delete unreadable files\n",
    "for root, _, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".edf\"):  # Only target .edf files\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Try opening the file to check if it is EDF-compliant\n",
    "                with pyedflib.EdfReader(file_path) as edf_file:\n",
    "                    pass  # File is readable\n",
    "            except:\n",
    "                # Log and remove unreadable files\n",
    "                unreadable_files.append(file_path)\n",
    "                os.remove(file_path)  # Delete the unreadable file\n",
    "\n",
    "# Save the list of unreadable files to a text file for reference\n",
    "unreadable_files_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/unreadable_files.txt'\n",
    "with open(unreadable_files_path, 'w') as f:\n",
    "    for file_path in unreadable_files:\n",
    "        f.write(f\"{file_path}\\n\")\n",
    "\n",
    "print(f\"Unreadable files have been removed. A list of unreadable files is saved to {unreadable_files_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42f97f6-6478-486c-91dc-8c93302c88d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the unreadable files log\n",
    "with open('C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/unreadable_files.txt', 'r') as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bacc60-66e5-4dd9-9df8-488add8901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata files\n",
    "metadata_sc = pd.read_excel('C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/SC-subjects.xls')\n",
    "metadata_st = pd.read_excel('C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/ST-subjects.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d5730b4-1624-4c24-964f-2d15a5b7c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in features_df: Index(['file_name', 'prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R',\n",
      "       'prop_?', 'total_duration'],\n",
      "      dtype='object')\n",
      "Columns in metadata_sc: Index(['subject', 'night', 'age', 'sex (F=1)', 'LightsOff'], dtype='object')\n",
      "Columns in metadata_st: Index(['Subject - age - sex', 'Unnamed: 1', 'Unnamed: 2', 'Placebo night',\n",
      "       'Unnamed: 4', 'Temazepam night', 'Unnamed: 6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in features_df:\", features_df.columns)\n",
    "print(\"Columns in metadata_sc:\", metadata_sc.columns)\n",
    "print(\"Columns in metadata_st:\", metadata_st.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb4cded-8938-448c-9642-132c616a64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.columns = features_df.columns.str.strip()\n",
    "metadata_sc.columns = metadata_sc.columns.str.strip()\n",
    "metadata_st.columns = metadata_st.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab9d9cf-f17a-49f9-8653-364bb5cd92d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                file_name     prop_W    prop_1     prop_2    prop_3    prop_4  \\\n",
      "0  SC4001EC-Hypnogram.edf  69.340278  2.013889   8.680556  3.506944  4.131944   \n",
      "1  SC4002EC-Hypnogram.edf  65.474123  2.049323  12.955887  3.265023  7.051059   \n",
      "2  SC4011EH-Hypnogram.edf  64.444444  3.784722  19.513889  3.333333  0.312500   \n",
      "3  SC4012EC-Hypnogram.edf  63.333333  3.194444  22.916667  2.777778  0.555556   \n",
      "4  SC4021EH-Hypnogram.edf  66.215278  3.263889  18.923611  2.534722  0.763889   \n",
      "\n",
      "     prop_R    prop_?  total_duration  \n",
      "0  4.340278  7.986111         86400.0  \n",
      "1  7.467871  1.736714         86370.0  \n",
      "2  5.902778  2.708333         86400.0  \n",
      "3  6.111111  1.111111         86400.0  \n",
      "4  5.659722  2.638889         86400.0  \n",
      "   subject  night  age  sex (F=1) LightsOff\n",
      "0        0      1   33          1  00:38:00\n",
      "1        0      2   33          1  21:57:00\n",
      "2        1      1   33          1  22:44:00\n",
      "3        1      2   33          1  22:15:00\n",
      "4        2      1   26          1  22:50:00\n",
      "  Subject - age - sex Unnamed: 1 Unnamed: 2 Placebo night  Unnamed: 4  \\\n",
      "0                  Nr        Age      M1/F2      night nr  lights off   \n",
      "1                   1         60          1             1    23:01:00   \n",
      "2                   2         35          2             2    23:27:00   \n",
      "3                   4         18          2             1    23:53:00   \n",
      "4                   5         32          2             2    23:23:00   \n",
      "\n",
      "  Temazepam night  Unnamed: 6  \n",
      "0        night nr  lights off  \n",
      "1               2    23:48:00  \n",
      "2               1    00:00:00  \n",
      "3               2    22:37:00  \n",
      "4               1    23:34:00  \n"
     ]
    }
   ],
   "source": [
    "print(features_df.head())\n",
    "print(metadata_sc.head())\n",
    "print(metadata_st.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e1377d2-cb8c-4e65-baff-0d332a4c9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'subject_id' in features_df\n",
      "Missing 'subject_id' in metadata_sc\n",
      "Missing 'subject_id' in metadata_st\n"
     ]
    }
   ],
   "source": [
    "if 'subject_id' not in features_df.columns:\n",
    "    print(\"Missing 'subject_id' in features_df\")\n",
    "if 'subject_id' not in metadata_sc.columns:\n",
    "    print(\"Missing 'subject_id' in metadata_sc\")\n",
    "if 'subject_id' not in metadata_st.columns:\n",
    "    print(\"Missing 'subject_id' in metadata_st\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f087665e-cab2-4b60-b946-582f10d0c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For features_df, extract subject_id from 'file_name'\n",
    "features_df['subject_id'] = features_df['file_name'].str.split('-').str[0]\n",
    "\n",
    "# For metadata_sc, use 'subject' as 'subject_id'\n",
    "metadata_sc.rename(columns={'subject': 'subject_id'}, inplace=True)\n",
    "\n",
    "# For metadata_st, use 'Subject - age - sex' as 'subject_id'\n",
    "metadata_st.rename(columns={'Subject - age - sex': 'subject_id'}, inplace=True)\n",
    "\n",
    "# Ensure all subject_ids are strings\n",
    "features_df['subject_id'] = features_df['subject_id'].astype(str)\n",
    "metadata_sc['subject_id'] = metadata_sc['subject_id'].astype(str)\n",
    "metadata_st['subject_id'] = metadata_st['subject_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5159ca57-60a7-4bb3-b4e9-fbf970f140be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features DataFrame Columns: Index(['file_name', 'prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R',\n",
      "       'prop_?', 'total_duration', 'subject_id'],\n",
      "      dtype='object')\n",
      "Metadata SC Columns: Index(['subject_id', 'night', 'age', 'sex (F=1)', 'LightsOff'], dtype='object')\n",
      "Metadata ST Columns: Index(['subject_id', 'Unnamed: 1', 'Unnamed: 2', 'Placebo night', 'Unnamed: 4',\n",
      "       'Temazepam night', 'Unnamed: 6'],\n",
      "      dtype='object')\n",
      "                file_name     prop_W    prop_1     prop_2    prop_3    prop_4  \\\n",
      "0  SC4001EC-Hypnogram.edf  69.340278  2.013889   8.680556  3.506944  4.131944   \n",
      "1  SC4002EC-Hypnogram.edf  65.474123  2.049323  12.955887  3.265023  7.051059   \n",
      "2  SC4011EH-Hypnogram.edf  64.444444  3.784722  19.513889  3.333333  0.312500   \n",
      "3  SC4012EC-Hypnogram.edf  63.333333  3.194444  22.916667  2.777778  0.555556   \n",
      "4  SC4021EH-Hypnogram.edf  66.215278  3.263889  18.923611  2.534722  0.763889   \n",
      "\n",
      "     prop_R    prop_?  total_duration subject_id  \n",
      "0  4.340278  7.986111         86400.0   SC4001EC  \n",
      "1  7.467871  1.736714         86370.0   SC4002EC  \n",
      "2  5.902778  2.708333         86400.0   SC4011EH  \n",
      "3  6.111111  1.111111         86400.0   SC4012EC  \n",
      "4  5.659722  2.638889         86400.0   SC4021EH  \n",
      "  subject_id  night  age  sex (F=1) LightsOff\n",
      "0          0      1   33          1  00:38:00\n",
      "1          0      2   33          1  21:57:00\n",
      "2          1      1   33          1  22:44:00\n",
      "3          1      2   33          1  22:15:00\n",
      "4          2      1   26          1  22:50:00\n",
      "  subject_id Unnamed: 1 Unnamed: 2 Placebo night  Unnamed: 4 Temazepam night  \\\n",
      "0         Nr        Age      M1/F2      night nr  lights off        night nr   \n",
      "1          1         60          1             1    23:01:00               2   \n",
      "2          2         35          2             2    23:27:00               1   \n",
      "3          4         18          2             1    23:53:00               2   \n",
      "4          5         32          2             2    23:23:00               1   \n",
      "\n",
      "   Unnamed: 6  \n",
      "0  lights off  \n",
      "1    23:48:00  \n",
      "2    00:00:00  \n",
      "3    22:37:00  \n",
      "4    23:34:00  \n"
     ]
    }
   ],
   "source": [
    "print(\"Features DataFrame Columns:\", features_df.columns)\n",
    "print(\"Metadata SC Columns:\", metadata_sc.columns)\n",
    "print(\"Metadata ST Columns:\", metadata_st.columns)\n",
    "\n",
    "print(features_df.head())\n",
    "print(metadata_sc.head())\n",
    "print(metadata_st.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83be66e-6bbb-44f1-bce5-732f061387fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data has been saved as 'aggregated_sleep_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Merge SC metadata with features\n",
    "sc_merged_df = features_df.merge(metadata_sc, on='subject_id', how='left')\n",
    "\n",
    "# Merge ST metadata with features\n",
    "st_merged_df = features_df.merge(metadata_st, on='subject_id', how='left')\n",
    "\n",
    "# Concatenate SC and ST merged DataFrames\n",
    "aggregated_df = pd.concat([sc_merged_df, st_merged_df], ignore_index=True)\n",
    "\n",
    "# Save the aggregated DataFrame\n",
    "aggregated_df.to_csv('C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/aggregated_sleep_data.csv', index=False)\n",
    "print(\"Aggregated data has been saved as 'aggregated_sleep_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654d1f35-6e8b-4f0d-8084-532cf41827f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated features by age and sex calculated:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_</th>\n",
       "      <th>sex_</th>\n",
       "      <th>prop_W_mean</th>\n",
       "      <th>prop_W_var</th>\n",
       "      <th>prop_1_mean</th>\n",
       "      <th>prop_1_var</th>\n",
       "      <th>prop_2_mean</th>\n",
       "      <th>prop_2_var</th>\n",
       "      <th>prop_3_mean</th>\n",
       "      <th>prop_3_var</th>\n",
       "      <th>prop_4_mean</th>\n",
       "      <th>prop_4_var</th>\n",
       "      <th>prop_R_mean</th>\n",
       "      <th>prop_R_var</th>\n",
       "      <th>prop_?_mean</th>\n",
       "      <th>prop_?_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age_, sex_, prop_W_mean, prop_W_var, prop_1_mean, prop_1_var, prop_2_mean, prop_2_var, prop_3_mean, prop_3_var, prop_4_mean, prop_4_var, prop_R_mean, prop_R_var, prop_?_mean, prop_?_var]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the 'sex (F=1)' column to 'sex'\n",
    "aggregated_df = aggregated_df.rename(columns={'sex (F=1)': 'sex'})\n",
    "\n",
    "# Group by age and sex to calculate mean and variance of each sleep stage proportion\n",
    "aggregated_features_by_age_sex = (\n",
    "    aggregated_df.groupby(['age', 'sex'])\n",
    "    .agg({\n",
    "        'prop_W': ['mean', 'var'],\n",
    "        'prop_1': ['mean', 'var'],\n",
    "        'prop_2': ['mean', 'var'],\n",
    "        'prop_3': ['mean', 'var'],\n",
    "        'prop_4': ['mean', 'var'],\n",
    "        'prop_R': ['mean', 'var'],\n",
    "        'prop_?': ['mean', 'var']\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the column names for easier handling\n",
    "aggregated_features_by_age_sex.columns = [\n",
    "    '_'.join(col).strip() if isinstance(col, tuple) else col for col in aggregated_features_by_age_sex.columns\n",
    "]\n",
    "\n",
    "print(\"Aggregated features by age and sex calculated:\")\n",
    "aggregated_features_by_age_sex.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "630b0f58-70bd-4a49-b9a1-1cf08fc91398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition features calculated:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_W_to_1</th>\n",
       "      <th>trans_1_to_2</th>\n",
       "      <th>trans_2_to_3</th>\n",
       "      <th>trans_3_to_2</th>\n",
       "      <th>trans_3_to_4</th>\n",
       "      <th>trans_4_to_3</th>\n",
       "      <th>trans_4_to_W</th>\n",
       "      <th>trans_W_to_3</th>\n",
       "      <th>trans_4_to_2</th>\n",
       "      <th>trans_3_to_1</th>\n",
       "      <th>...</th>\n",
       "      <th>trans_1_to_1</th>\n",
       "      <th>trans_4_to_R</th>\n",
       "      <th>trans_?_to_R</th>\n",
       "      <th>trans_1_to_?</th>\n",
       "      <th>trans_?_to_1</th>\n",
       "      <th>trans_3_to_?</th>\n",
       "      <th>trans_?_to_2</th>\n",
       "      <th>trans_2_to_?</th>\n",
       "      <th>trans_R_to_?</th>\n",
       "      <th>trans_W_to_W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trans_W_to_1  trans_1_to_2  trans_2_to_3  trans_3_to_2  trans_3_to_4  \\\n",
       "0            10            14          28.0          23.0          22.0   \n",
       "1            22            20          16.0          12.0          16.0   \n",
       "2            14            23          14.0          12.0           8.0   \n",
       "3            16            30          21.0          21.0           7.0   \n",
       "4             6            18          31.0          27.0          13.0   \n",
       "\n",
       "   trans_4_to_3  trans_4_to_W  trans_W_to_3  trans_4_to_2  trans_3_to_1  ...  \\\n",
       "0          18.0           1.0           1.0           2.0           2.0  ...   \n",
       "1          14.0           NaN           NaN           1.0           NaN  ...   \n",
       "2           7.0           NaN           NaN           1.0           1.0  ...   \n",
       "3           8.0           NaN           NaN           1.0           NaN  ...   \n",
       "4          12.0           NaN           NaN           3.0           1.0  ...   \n",
       "\n",
       "   trans_1_to_1  trans_4_to_R  trans_?_to_R  trans_1_to_?  trans_?_to_1  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN   \n",
       "1           NaN           NaN           NaN           NaN           NaN   \n",
       "2           NaN           NaN           NaN           NaN           NaN   \n",
       "3           NaN           NaN           NaN           NaN           NaN   \n",
       "4           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "   trans_3_to_?  trans_?_to_2  trans_2_to_?  trans_R_to_?  trans_W_to_W  \n",
       "0           NaN           NaN           NaN           NaN           NaN  \n",
       "1           NaN           NaN           NaN           NaN           NaN  \n",
       "2           NaN           NaN           NaN           NaN           NaN  \n",
       "3           NaN           NaN           NaN           NaN           NaN  \n",
       "4           NaN           NaN           NaN           NaN           NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate transition frequencies\n",
    "def calculate_transitions(annotations):\n",
    "    transitions = defaultdict(int)\n",
    "    previous_stage = None\n",
    "    for onset, duration, annotation in zip(*annotations):\n",
    "        if \"Sleep stage\" in annotation:\n",
    "            current_stage = annotation.split()[-1]\n",
    "            if previous_stage:\n",
    "                transitions[(previous_stage, current_stage)] += 1\n",
    "            previous_stage = current_stage\n",
    "    return transitions\n",
    "\n",
    "# Apply transition calculation to each hypnogram file\n",
    "transition_features_list = []\n",
    "for hypnogram_path in hypnogram_files:\n",
    "    try:\n",
    "        with pyedflib.EdfReader(hypnogram_path) as hypnogram:\n",
    "            annotations = hypnogram.readAnnotations()\n",
    "            transitions = calculate_transitions(annotations)\n",
    "\n",
    "            # Flatten transition counts for the DataFrame\n",
    "            transition_data = {f\"trans_{k[0]}_to_{k[1]}\": v for k, v in transitions.items()}\n",
    "            transition_data['file_name'] = os.path.basename(hypnogram_path)\n",
    "            transition_features_list.append(transition_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file {hypnogram_path}: {e}\")\n",
    "\n",
    "# Convert list of transitions to a DataFrame\n",
    "transition_features_df = pd.DataFrame(transition_features_list)\n",
    "print(\"Transition features calculated:\")\n",
    "transition_features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9722f9d4-18f3-436a-9401-1bb199b882b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep quality indicators (REM and deep sleep ratios) calculated:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>REM_ratio</th>\n",
       "      <th>Deep_sleep_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SC4001EC-Hypnogram.edf</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC4002EC-Hypnogram.edf</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SC4011EH-Hypnogram.edf</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SC4012EC-Hypnogram.edf</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SC4021EH-Hypnogram.edf</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file_name  REM_ratio  Deep_sleep_ratio\n",
       "0  SC4001EC-Hypnogram.edf   0.000050          0.000048\n",
       "1  SC4002EC-Hypnogram.edf   0.000086          0.000082\n",
       "2  SC4011EH-Hypnogram.edf   0.000068          0.000004\n",
       "3  SC4012EC-Hypnogram.edf   0.000071          0.000006\n",
       "4  SC4021EH-Hypnogram.edf   0.000066          0.000009"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate REM and deep sleep ratios\n",
    "features_df['REM_ratio'] = features_df['prop_R'] / features_df['total_duration']\n",
    "features_df['Deep_sleep_ratio'] = features_df['prop_4'] / features_df['total_duration']\n",
    "\n",
    "# Display the first few rows to verify calculations\n",
    "print(\"Sleep quality indicators (REM and deep sleep ratios) calculated:\")\n",
    "features_df[['file_name', 'REM_ratio', 'Deep_sleep_ratio']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b200fd-f528-476b-af25-4d3db0d33f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated DataFrame with emotion scores:\n",
      "                file_name    prop_R    prop_3    prop_4    prop_1     prop_2  \\\n",
      "0  SC4001EC-Hypnogram.edf  4.340278  3.506944  4.131944  2.013889   8.680556   \n",
      "1  SC4002EC-Hypnogram.edf  7.467871  3.265023  7.051059  2.049323  12.955887   \n",
      "2  SC4011EH-Hypnogram.edf  5.902778  3.333333  0.312500  3.784722  19.513889   \n",
      "3  SC4012EC-Hypnogram.edf  6.111111  2.777778  0.555556  3.194444  22.916667   \n",
      "4  SC4021EH-Hypnogram.edf  5.659722  2.534722  0.763889  3.263889  18.923611   \n",
      "\n",
      "   contentment  stress_relief      calm  relaxation  mild_alertness  \\\n",
      "0     2.170139       2.170139  4.583333    3.055556        7.486111   \n",
      "1     3.733935       3.733935  6.189649    4.126433       10.503647   \n",
      "2     2.951389       2.951389  2.187500    1.458333       16.309028   \n",
      "3     3.055556       3.055556  2.000000    1.333333       18.277778   \n",
      "4     2.829861       2.829861  1.979167    1.319444       15.531250   \n",
      "\n",
      "   low_energy  \n",
      "0    3.208333  \n",
      "1    4.501563  \n",
      "2    6.989583  \n",
      "3    7.833333  \n",
      "4    6.656250  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample emotion mapping based on research (adjust as needed)\n",
    "emotion_mapping = {\n",
    "    'REM': {'contentment': 0.5, 'stress_relief': 0.5},\n",
    "    'Deep': {'calm': 0.6, 'relaxation': 0.4},\n",
    "    'Light': {'mild_alertness': 0.7, 'low_energy': 0.3}\n",
    "}\n",
    "\n",
    "# Function to calculate emotion scores based on sleep stage proportions\n",
    "def calculate_emotion_scores(row, emotion_mapping):\n",
    "    scores = {'contentment': 0, 'stress_relief': 0, 'calm': 0, 'relaxation': 0, 'mild_alertness': 0, 'low_energy': 0}\n",
    "\n",
    "    # Calculate weighted emotion scores for each stage\n",
    "    for stage, emotions in emotion_mapping.items():\n",
    "        if stage == 'REM':\n",
    "            for emotion, weight in emotions.items():\n",
    "                scores[emotion] += row['prop_R'] * weight\n",
    "        elif stage == 'Deep':\n",
    "            for emotion, weight in emotions.items():\n",
    "                scores[emotion] += (row['prop_3'] + row['prop_4']) * weight\n",
    "        elif stage == 'Light':\n",
    "            for emotion, weight in emotions.items():\n",
    "                scores[emotion] += (row['prop_1'] + row['prop_2']) * weight\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Apply emotion score calculation for each record in the DataFrame\n",
    "aggregated_df['emotion_scores'] = aggregated_df.apply(lambda row: calculate_emotion_scores(row, emotion_mapping), axis=1)\n",
    "\n",
    "# Expand the emotion scores into individual columns\n",
    "emotion_scores_df = pd.DataFrame(aggregated_df['emotion_scores'].tolist())\n",
    "aggregated_df = pd.concat([aggregated_df, emotion_scores_df], axis=1)\n",
    "\n",
    "# Drop the temporary 'emotion_scores' column after expansion\n",
    "aggregated_df.drop(columns=['emotion_scores'], inplace=True)\n",
    "\n",
    "# Display the final DataFrame with emotion scores\n",
    "print(\"Aggregated DataFrame with emotion scores:\")\n",
    "print(aggregated_df[['file_name', 'prop_R', 'prop_3', 'prop_4', 'prop_1', 'prop_2', 'contentment', 'stress_relief', 'calm', 'relaxation', 'mild_alertness', 'low_energy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad5a4eb5-d682-412f-b5c4-3529504b353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.88      1.00      0.93        14\n",
      "     sadness       1.00      1.00      1.00        71\n",
      "       anger       1.00      1.00      1.00        76\n",
      "    surprise       0.97      1.00      0.99        68\n",
      "        fear       1.00      1.00      1.00        76\n",
      "\n",
      "   micro avg       0.99      1.00      0.99       305\n",
      "   macro avg       0.97      1.00      0.98       305\n",
      "weighted avg       0.99      1.00      0.99       305\n",
      " samples avg       0.99      1.00      0.99       305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the features (sleep stage proportions and metadata)\n",
    "feature_columns = ['prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R']\n",
    "X = aggregated_df[feature_columns]\n",
    "\n",
    "# Define target emotions (synthetic labels created based on sleep feature scores)\n",
    "emotion_labels = ['happiness', 'sadness', 'anger', 'surprise', 'fear']\n",
    "aggregated_df['happiness'] = np.clip(aggregated_df['contentment'] + aggregated_df['relaxation'] - aggregated_df['low_energy'], 0, 1)\n",
    "aggregated_df['sadness'] = np.clip(aggregated_df['low_energy'] - aggregated_df['contentment'], 0, 1)\n",
    "aggregated_df['anger'] = np.clip(aggregated_df['mild_alertness'] - aggregated_df['calm'], 0, 1)\n",
    "aggregated_df['surprise'] = np.clip(aggregated_df['stress_relief'] - aggregated_df['relaxation'], 0, 1)\n",
    "aggregated_df['fear'] = np.clip(aggregated_df['stress_relief'] + aggregated_df['low_energy'], 0, 1)\n",
    "\n",
    "# Binarize the continuous labels by setting a threshold\n",
    "y = (aggregated_df[emotion_labels] > 0.5).astype(int)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model with a classification report\n",
    "print(\"Multi-label Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=emotion_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b9584b3-e250-4dea-8d73-a68e5ae19b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion probabilities (as percentages) for the test set:\n",
      "    Person_ID  happiness  sadness  anger  surprise   fear\n",
      "0         266        4.0     96.0  100.0     100.0  100.0\n",
      "1         261        3.0    100.0  100.0      81.0  100.0\n",
      "2         265        0.0    100.0  100.0     100.0  100.0\n",
      "3          39       15.0     99.0  100.0      92.0  100.0\n",
      "4          33       96.0     17.0  100.0      92.0  100.0\n",
      "..        ...        ...      ...    ...       ...    ...\n",
      "71        316       47.0     91.0  100.0      97.0  100.0\n",
      "72         63        7.0     97.0  100.0     100.0  100.0\n",
      "73        229       15.0     99.0  100.0      92.0  100.0\n",
      "74         82        5.0     97.0  100.0      21.0  100.0\n",
      "75         94        0.0    100.0  100.0     100.0  100.0\n",
      "\n",
      "[76 rows x 6 columns]\n",
      "\n",
      "Predicted emotion probabilities saved to C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/test_set_emotion_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Convert probabilities to percentages and create a DataFrame\n",
    "y_proba_percent = pd.DataFrame(\n",
    "    {\n",
    "        emotion: [probs[1] * 100 if probs.shape[0] > 1 else probs[0] * 100 for probs in proba]  # Handle single-column case\n",
    "        for emotion, proba in zip(emotion_labels, y_proba)\n",
    "    },\n",
    "    columns=emotion_labels\n",
    ")\n",
    "\n",
    "# Add the corresponding Person IDs or indices from the test set\n",
    "y_proba_percent['Person_ID'] = X_test.index\n",
    "\n",
    "# Rearrange columns for better readability\n",
    "y_proba_percent = y_proba_percent[['Person_ID'] + emotion_labels]\n",
    "\n",
    "# Display the predicted emotions with percentages\n",
    "print(\"Predicted emotion probabilities (as percentages) for the test set:\")\n",
    "print(y_proba_percent)\n",
    "\n",
    "# Save the predictions to a CSV for further analysis\n",
    "output_path = \"C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/test_set_emotion_predictions.csv\"\n",
    "y_proba_percent.to_csv(output_path, index=False)\n",
    "print(f\"\\nPredicted emotion probabilities saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd184412-1695-41c8-8f40-e9c3aace09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Training and testing sets created.\n",
      "Best model parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Model training completed!\n",
      "Predicted emotion probabilities (as percentages) for the test set:\n",
      "   Index  happiness  sadness  anger  surprise   fear\n",
      "0    266        5.0     94.0    0.0     100.0  100.0\n",
      "1    261       80.0     80.0    0.0     100.0  100.0\n",
      "2    265      100.0      0.0    0.0     100.0  100.0\n",
      "3     39       84.0     12.0    0.0     100.0  100.0\n",
      "4     33      100.0      0.0    1.0     100.0  100.0\n",
      "\n",
      "Predicted emotion probabilities saved to C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/aggregated_emotion_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/aggregated_sleep_data.csv\"\n",
    "aggregated_df = pd.read_csv(file_path)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Select relevant features (sleep stage proportions)\n",
    "feature_columns = ['prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R', 'prop_?']\n",
    "X = aggregated_df[feature_columns]\n",
    "\n",
    "# Generate synthetic emotion labels based on sleep stage proportions\n",
    "# Adjust the logic here if you have specific mappings for emotions\n",
    "emotion_labels = ['happiness', 'sadness', 'anger', 'surprise', 'fear']\n",
    "aggregated_df['happiness'] = np.clip(aggregated_df['prop_R'] - aggregated_df['prop_1'], 0, 1)\n",
    "aggregated_df['sadness'] = np.clip(aggregated_df['prop_?'] - aggregated_df['prop_R'], 0, 1)\n",
    "aggregated_df['anger'] = np.clip(aggregated_df['prop_3'] - aggregated_df['prop_W'], 0, 1)\n",
    "aggregated_df['surprise'] = np.clip(aggregated_df['prop_4'] - aggregated_df['prop_2'], 0, 1)\n",
    "aggregated_df['fear'] = np.clip(aggregated_df['prop_?'] + aggregated_df['prop_1'], 0, 1)\n",
    "\n",
    "# Binarize the labels (threshold 0.5)\n",
    "y = (aggregated_df[emotion_labels] > 0.5).astype(int)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training and testing sets created.\")\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1_micro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "model = grid_search.best_estimator_\n",
    "print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the Random Forest model\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Convert probabilities to percentages and create a DataFrame\n",
    "# Adjusted to handle single-class probabilities\n",
    "y_proba_percent = pd.DataFrame(\n",
    "    {\n",
    "        emotion: [\n",
    "            probs[1] * 100 if probs.shape[0] > 1 else probs[0] * 100\n",
    "            for probs in proba\n",
    "        ]\n",
    "        for emotion, proba in zip(emotion_labels, y_proba)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add the corresponding indices from the test set\n",
    "y_proba_percent['Index'] = aggregated_df.iloc[y_test.index].index\n",
    "\n",
    "# Rearrange columns for better readability\n",
    "y_proba_percent = y_proba_percent[['Index'] + emotion_labels]\n",
    "\n",
    "# Display the predicted emotions with percentages\n",
    "print(\"Predicted emotion probabilities (as percentages) for the test set:\")\n",
    "print(y_proba_percent.head())\n",
    "\n",
    "# Save the predictions to a CSV for further analysis\n",
    "output_path = \"C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/aggregated_emotion_predictions.csv\"\n",
    "y_proba_percent.to_csv(output_path, index=False)\n",
    "print(f\"\\nPredicted emotion probabilities saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd34453-107f-488e-a092-9c891ee92254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Training and testing sets created.\n",
      "Best model parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Model training completed!\n",
      "Normalized emotion probabilities (sum to 100%):\n",
      "    happiness    sadness      anger   surprise       fear  Index\n",
      "0    5.131787  23.717053  23.717053  23.717053  23.717053    266\n",
      "1    2.662052  23.477174  24.620258  24.620258  24.620258    261\n",
      "2   19.472706  20.132706  20.132706  20.132706  20.129174    265\n",
      "3   17.069864  17.321675  21.869487  21.869487  21.869487     39\n",
      "4   21.436076   1.852923  25.655853  25.655853  25.399295     33\n",
      "..        ...        ...        ...        ...        ...    ...\n",
      "71  14.438154  18.481309  22.360179  22.360179  22.360179    316\n",
      "72  15.699989  21.075927  21.075927  21.075927  21.072230     63\n",
      "73  17.069864  17.321675  21.869487  21.869487  21.869487    229\n",
      "74   1.271583  24.655650  24.690923  24.690923  24.690923     82\n",
      "75  18.023068  20.495132  20.495132  20.495132  20.491536     94\n",
      "\n",
      "[76 rows x 6 columns]\n",
      "Normalized emotion probabilities saved to C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/normalized_emotion_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/aggregated_sleep_data.csv\"\n",
    "sleep_data_df = pd.read_csv(dataset_path)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Select relevant features\n",
    "selected_features = ['prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R']\n",
    "X = sleep_data_df[selected_features]\n",
    "\n",
    "# Generate synthetic emotion labels for demonstration\n",
    "emotion_labels = ['happiness', 'sadness', 'anger', 'surprise', 'fear']\n",
    "sleep_data_df['happiness'] = np.clip(sleep_data_df['prop_R'] - sleep_data_df['prop_?'], 0, 1)\n",
    "sleep_data_df['sadness'] = np.clip(sleep_data_df['prop_1'] - sleep_data_df['prop_4'], 0, 1)\n",
    "sleep_data_df['anger'] = np.clip(sleep_data_df['prop_3'] - sleep_data_df['prop_2'], 0, 1)\n",
    "sleep_data_df['surprise'] = np.clip(sleep_data_df['prop_2'] + sleep_data_df['prop_R'], 0, 1)\n",
    "sleep_data_df['fear'] = np.clip(sleep_data_df['prop_W'] - sleep_data_df['prop_1'], 0, 1)\n",
    "\n",
    "# Binarize the labels for multi-label classification\n",
    "y = (sleep_data_df[emotion_labels] > 0.5).astype(int)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training and testing sets created.\")\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1_micro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "model = grid_search.best_estimator_\n",
    "print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Normalize probabilities to sum up to 100% for each individual\n",
    "normalized_proba = []\n",
    "for i in range(len(X_test)):\n",
    "    probabilities = []\n",
    "    for proba in y_proba:\n",
    "        # Handle cases with only one class in probabilities\n",
    "        if len(proba[i]) > 1:\n",
    "            probabilities.append(proba[i, 1])  # Class 1 probability\n",
    "        else:\n",
    "            probabilities.append(proba[i, 0])  # Single class probability\n",
    "    total = sum(probabilities)\n",
    "    normalized = (np.array(probabilities) / total) * 100 if total > 0 else probabilities\n",
    "    normalized_proba.append(normalized)\n",
    "\n",
    "# Convert to DataFrame\n",
    "normalized_proba_df = pd.DataFrame(normalized_proba, columns=emotion_labels)\n",
    "\n",
    "# Add Person IDs or indices\n",
    "normalized_proba_df['Index'] = sleep_data_df.iloc[X_test.index].index\n",
    "\n",
    "# Display the normalized probabilities\n",
    "print(\"Normalized emotion probabilities (sum to 100%):\")\n",
    "print(normalized_proba_df)\n",
    "\n",
    "# Save the predictions\n",
    "output_path = \"C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/normalized_emotion_predictions.csv\"\n",
    "normalized_proba_df.to_csv(output_path, index=False)\n",
    "print(f\"Normalized emotion probabilities saved to {output_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e77645ac-5d0a-48c1-9729-3087a8d5147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize predicted probabilities using a threshold\n",
    "threshold = 50\n",
    "y_pred_binarized = (normalized_proba_df.iloc[:, :-1].values >= threshold).astype(int)  # Exclude 'Index' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f656bfe9-fe8a-452e-a0c2-698556c38748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in training set:\n",
      "happiness    195\n",
      "sadness      222\n",
      "anger          0\n",
      "surprise     304\n",
      "fear         274\n",
      "dtype: int64\n",
      "\n",
      "Label distribution in testing set:\n",
      "happiness    53\n",
      "sadness      60\n",
      "anger         0\n",
      "surprise     76\n",
      "fear         70\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Label distribution in training set:\")\n",
    "print(y_train.sum(axis=0))  # Sum of each label in training set\n",
    "print(\"\\nLabel distribution in testing set:\")\n",
    "print(y_test.sum(axis=0))  # Sum of each label in testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac526d16-9a42-4867-a6ed-c24e5442e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing label shape: (304, 5)\n",
      "Updated training dataset shape:\n",
      "Features: (354, 6)\n",
      "Labels: (354, 5)\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of existing labels\n",
    "print(\"Existing label shape:\", y_train.shape)\n",
    "\n",
    "# Add missing columns in y_train for alignment (if necessary)\n",
    "for emotion in emotion_labels:\n",
    "    if emotion not in y_train.columns:\n",
    "        y_train[emotion] = 0  # Initialize with 0 for missing labels\n",
    "\n",
    "# Generate synthetic features (if not already defined)\n",
    "num_samples = 50  # Number of synthetic samples to generate\n",
    "synthetic_features = pd.DataFrame(\n",
    "    np.random.rand(num_samples, X_train.shape[1]),\n",
    "    columns=X_train.columns\n",
    ")\n",
    "\n",
    "# Expand synthetic labels for all emotions\n",
    "synthetic_labels_full = pd.DataFrame({\n",
    "    'happiness': [0] * num_samples,\n",
    "    'sadness': [0] * num_samples,\n",
    "    'anger': [1] * num_samples,       # Anger is 1 for synthetic data\n",
    "    'surprise': [0] * num_samples,\n",
    "    'fear': [0] * num_samples\n",
    "})\n",
    "\n",
    "# Append synthetic data to training set\n",
    "X_train = pd.concat([X_train, synthetic_features], ignore_index=True)\n",
    "y_train = pd.concat([y_train, synthetic_labels_full], ignore_index=True)\n",
    "\n",
    "# Verify updated training dataset shapes\n",
    "print(\"Updated training dataset shape:\")\n",
    "print(\"Features:\", X_train.shape)\n",
    "print(\"Labels:\", y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd2de32-83f9-484b-9b5d-9540f8828c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-split dataset shapes:\n",
      "Features: (304, 6)\n",
      "Labels: (304, 5)\n",
      "Synthetic features shape: (50, 6)\n",
      "Synthetic labels shape: (50, 5)\n",
      "Updated dataset shape:\n",
      "Features: (354, 6)\n",
      "Labels: (354, 5)\n"
     ]
    }
   ],
   "source": [
    "# Che# Split data again to ensure initial alignment if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Post-split dataset shapes:\")\n",
    "print(\"Features:\", X_train.shape)\n",
    "print(\"Labels:\", y_train.shape)\n",
    "\n",
    "# Check if we need to add synthetic data\n",
    "print(\"Synthetic features shape:\", synthetic_features.shape)\n",
    "print(\"Synthetic labels shape:\", synthetic_labels_full.shape)\n",
    "\n",
    "# Append synthetic data if initial shapes align\n",
    "if X_train.shape[0] == y_train.shape[0] and synthetic_features.shape[0] == synthetic_labels_full.shape[0]:\n",
    "    X_train = pd.concat([X_train, synthetic_features], ignore_index=True)\n",
    "    y_train = pd.concat([y_train, synthetic_labels_full], ignore_index=True)\n",
    "else:\n",
    "    print(\"Error: Initial mismatch in data. Verify the dataset generation and split process.\")\n",
    "\n",
    "# Final shape check\n",
    "print(\"Updated dataset shape:\")\n",
    "print(\"Features:\", X_train.shape)\n",
    "print(\"Labels:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51c9a798-7c71-4a7c-abdf-639a0eacd7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model retraining completed!\n",
      "\n",
      "Emotion Percentages for the Test Set:\n",
      "happiness: 60.56%\n",
      "sadness: 54.93%\n",
      "anger: 14.08%\n",
      "surprise: 85.92%\n",
      "fear: 80.28%\n",
      "Normalized emotion predictions saved to C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/normalized_emotion_predictions_updated.csv.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Combine original and synthetic data (already done in previous steps)\n",
    "# Updated X_train and y_train with both original and synthetic data\n",
    "\n",
    "# Split combined data into new training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model with hyperparameters from previous best GridSearch\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model retraining completed!\")\n",
    "\n",
    "# Make predictions on the test set and get probability estimates\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate emotion percentages for the entire test set\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=emotion_labels)\n",
    "emotion_percentages = (y_pred_df.sum() / len(y_pred_df)) * 100\n",
    "\n",
    "print(\"\\nEmotion Percentages for the Test Set:\")\n",
    "for emotion, percentage in emotion_percentages.items():\n",
    "    print(f\"{emotion}: {percentage:.2f}%\")\n",
    "\n",
    "# Save normalized probabilities to a CSV file\n",
    "output_path = \"C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/normalized_emotion_predictions_updated.csv\"\n",
    "y_pred_df.to_csv(output_path, index=False)\n",
    "print(f\"Normalized emotion predictions saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74ee09c8-675a-41b6-a4f1-d246627b14c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in training set:\n",
      "happiness    152\n",
      "sadness      182\n",
      "anger         40\n",
      "surprise     243\n",
      "fear         217\n",
      "dtype: int64\n",
      "\n",
      "Label distribution in testing set:\n",
      "happiness    43\n",
      "sadness      40\n",
      "anger        10\n",
      "surprise     61\n",
      "fear         57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label distribution in training and testing datasets\n",
    "print(\"Label distribution in training set:\")\n",
    "print(y_train.sum(axis=0))  # Sum of each label in training set\n",
    "\n",
    "print(\"\\nLabel distribution in testing set:\")\n",
    "print(y_test.sum(axis=0))  # Sum of each label in testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba8450-f63e-4a95-b967-ddcdc0609e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc55caad-289e-4abc-837c-0a393676534b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (71,5) (76,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Identify misclassified samples\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m misclassified_indices \u001b[38;5;241m=\u001b[39m (\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred_binarized\u001b[49m)\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m misclassified_samples \u001b[38;5;241m=\u001b[39m X_test[misclassified_indices]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display some misclassified samples\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (71,5) (76,5) "
     ]
    }
   ],
   "source": [
    "# Identify misclassified samples\n",
    "misclassified_indices = (y_test.values != y_pred_binarized).any(axis=1)\n",
    "misclassified_samples = X_test[misclassified_indices]\n",
    "\n",
    "# Display some misclassified samples\n",
    "print(\"Misclassified samples:\")\n",
    "print(misclassified_samples.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "725b1d5c-b61c-425f-9cb6-95685388afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (71, 5)\n",
      "y_pred_binarized shape: (76, 5)\n",
      "Adjusted y_test shape: (71, 5)\n",
      "Adjusted y_pred_binarized shape: (71, 5)\n",
      "Misclassified samples:\n",
      "        prop_W    prop_1     prop_2    prop_3     prop_4     prop_R\n",
      "220   9.734513  5.309735  55.044248  7.079646   3.716814  19.115044\n",
      "42   66.516151  0.729420  11.392845  6.078499   1.389371   9.100382\n",
      "286  65.138889  3.333333  16.631944  2.222222   0.034722   7.430556\n",
      "181  67.291667  5.069444  18.055556  3.611111   0.416667   5.555556\n",
      "56    5.612245  3.061224  26.326531  8.673469  21.326531  35.000000\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of y_test and y_pred_binarized\n",
    "print(\"y_test shape:\", y_test.values.shape)\n",
    "print(\"y_pred_binarized shape:\", y_pred_binarized.shape)\n",
    "\n",
    "# Adjust shapes if there's a mismatch\n",
    "if len(y_test) > len(y_pred_binarized):\n",
    "    y_test = y_test.iloc[:len(y_pred_binarized)]\n",
    "elif len(y_pred_binarized) > len(y_test):\n",
    "    y_pred_binarized = y_pred_binarized[:len(y_test)]\n",
    "\n",
    "# Ensure the shapes are now consistent\n",
    "print(\"Adjusted y_test shape:\", y_test.values.shape)\n",
    "print(\"Adjusted y_pred_binarized shape:\", y_pred_binarized.shape)\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified_indices = (y_test.values != y_pred_binarized).any(axis=1)\n",
    "misclassified_samples = X_test.iloc[misclassified_indices]\n",
    "\n",
    "# Display some misclassified samples\n",
    "print(\"Misclassified samples:\")\n",
    "print(misclassified_samples.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8846ac60-6d5e-47d6-9ef7-3e565d29216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores (F1): [0.99801193 0.99807322 1.         0.9943074  0.99796334]\n",
      "Mean CV Score: 0.9977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation on the full dataset\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1_micro')\n",
    "print(f\"Cross-Validation Scores (F1): {cv_scores}\")\n",
    "print(f\"Mean CV Score: {np.mean(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76fdc81e-186e-495b-9d03-59672e1e38fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (No Synthetic Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.93      0.95      0.94        43\n",
      "     sadness       0.97      0.97      0.97        40\n",
      "       anger       1.00      1.00      1.00        10\n",
      "    surprise       1.00      1.00      1.00        61\n",
      "        fear       1.00      0.98      0.99        57\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       211\n",
      "   macro avg       0.98      0.98      0.98       211\n",
      "weighted avg       0.98      0.98      0.98       211\n",
      " samples avg       0.98      0.99      0.98       211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model without synthetic data\n",
    "X_train_no_synthetic = X_train.iloc[:-num_samples]\n",
    "y_train_no_synthetic = y_train.iloc[:-num_samples]\n",
    "\n",
    "model_no_synthetic = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=2, random_state=42\n",
    ")\n",
    "model_no_synthetic.fit(X_train_no_synthetic, y_train_no_synthetic)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_no_synthetic = model_no_synthetic.predict(X_test)\n",
    "print(\"Classification Report (No Synthetic Data):\")\n",
    "print(classification_report(y_test, y_pred_no_synthetic, target_names=emotion_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c099dbf2-9f46-4edf-8f69-e35913046ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Features (first 5 rows):\n",
      "     prop_W    prop_1    prop_2    prop_3    prop_4    prop_R    prop_?\n",
      "0  0.881679  0.078217  0.020451  0.201858  0.193747  0.102642  0.273810\n",
      "1  0.831202  0.079991  0.089197  0.187933  0.330624  0.194182  0.059544\n",
      "2  0.817758  0.166872  0.194647  0.191865  0.014653  0.148374  0.092857\n",
      "3  0.803251  0.137321  0.249363  0.159888  0.026050  0.154472  0.038095\n",
      "4  0.840878  0.140797  0.185156  0.145897  0.035819  0.141260  0.090476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Choose normalization method: Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply scaler to feature columns\n",
    "normalized_features = scaler.fit_transform(aggregated_df[feature_columns])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_features, columns=feature_columns)\n",
    "\n",
    "# Add normalized features back to the main DataFrame\n",
    "aggregated_df[feature_columns] = normalized_df\n",
    "\n",
    "print(\"Normalized Features (first 5 rows):\")\n",
    "print(aggregated_df[feature_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99723ddb-3164-4309-a069-4dc07850898c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prop_W_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'prop_W_log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Replace original features with log-transformed ones\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m aggregated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_W\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43maggregated_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprop_W_log\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m aggregated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aggregated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_2_log\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m aggregated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_R\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aggregated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_R_log\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'prop_W_log'"
     ]
    }
   ],
   "source": [
    "# Replace original features with log-transformed ones\n",
    "aggregated_df['prop_W'] = aggregated_df['prop_W_log']\n",
    "aggregated_df['prop_2'] = aggregated_df['prop_2_log']\n",
    "aggregated_df['prop_R'] = aggregated_df['prop_R_log']\n",
    "\n",
    "# Drop the log-transformed temporary columns\n",
    "aggregated_df.drop(['prop_W_log', 'prop_2_log', 'prop_R_log'], axis=1, inplace=True)\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "feature_columns = ['prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R']\n",
    "X = aggregated_df[feature_columns]\n",
    "\n",
    "# Define labels (emotions)\n",
    "emotion_labels = ['happiness', 'sadness', 'anger', 'surprise', 'fear']\n",
    "y = (aggregated_df[emotion_labels] > 0.5).astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Updated training and testing datasets are ready!\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd6ff2a1-5589-484f-bc75-047744f14608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns: ['prop_W_log', 'prop_2_log', 'prop_R_log']. Creating them using log transformation.\n",
      "Updated training and testing datasets are ready!\n",
      "X_train shape: (304, 6)\n",
      "y_train shape: (304, 5)\n",
      "X_test shape: (76, 6)\n",
      "y_test shape: (76, 5)\n"
     ]
    }
   ],
   "source": [
    "# Check if log-transformed columns exist\n",
    "required_columns = ['prop_W_log', 'prop_2_log', 'prop_R_log']\n",
    "missing_columns = [col for col in required_columns if col not in aggregated_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}. Creating them using log transformation.\")\n",
    "    import numpy as np\n",
    "    if 'prop_W' in aggregated_df.columns:\n",
    "        aggregated_df['prop_W_log'] = np.log1p(aggregated_df['prop_W'])\n",
    "    if 'prop_2' in aggregated_df.columns:\n",
    "        aggregated_df['prop_2_log'] = np.log1p(aggregated_df['prop_2'])\n",
    "    if 'prop_R' in aggregated_df.columns:\n",
    "        aggregated_df['prop_R_log'] = np.log1p(aggregated_df['prop_R'])\n",
    "\n",
    "# Replace original features with log-transformed ones\n",
    "aggregated_df['prop_W'] = aggregated_df['prop_W_log']\n",
    "aggregated_df['prop_2'] = aggregated_df['prop_2_log']\n",
    "aggregated_df['prop_R'] = aggregated_df['prop_R_log']\n",
    "\n",
    "# Drop the log-transformed temporary columns\n",
    "aggregated_df.drop(['prop_W_log', 'prop_2_log', 'prop_R_log'], axis=1, inplace=True)\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "feature_columns = ['prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R']\n",
    "X = aggregated_df[feature_columns]\n",
    "\n",
    "# Define labels (emotions)\n",
    "emotion_labels = ['happiness', 'sadness', 'anger', 'surprise', 'fear']\n",
    "y = (aggregated_df[emotion_labels] > 0.5).astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Updated training and testing datasets are ready!\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc5e8312-29d6-47e6-8214-c2e8cacf2309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Label Distribution (%):\n",
      "happiness    0.657895\n",
      "sadness      0.263158\n",
      "anger        0.085526\n",
      "surprise     0.000000\n",
      "fear         1.000000\n",
      "dtype: float64\n",
      "\n",
      "Testing Set Label Distribution (%):\n",
      "happiness    0.684211\n",
      "sadness      0.263158\n",
      "anger        0.026316\n",
      "surprise     0.000000\n",
      "fear         1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of each label in the dataset\n",
    "label_distribution_train = y_train.sum(axis=0) / len(y_train)\n",
    "label_distribution_test = y_test.sum(axis=0) / len(y_test)\n",
    "\n",
    "print(\"Training Set Label Distribution (%):\")\n",
    "print(label_distribution_train)\n",
    "print(\"\\nTesting Set Label Distribution (%):\")\n",
    "print(label_distribution_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30e79b12-cbc5-4a23-9e08-cf671a510c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in training set (per label):\n",
      "happiness    200\n",
      "sadness       80\n",
      "anger         26\n",
      "surprise       0\n",
      "fear         304\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Label distribution in training set (per label):\")\n",
    "print(y_train.sum(axis=0))  # This will show the count of 1s for each label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49247b9c-1b29-43f7-842c-680d577fe605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Set Label Distribution:\n",
      "happiness    200.0\n",
      "sadness      176.0\n",
      "anger        122.0\n",
      "surprise       0.0\n",
      "fear         304.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Create balanced dataset for each label separately\n",
    "X_train_balanced = X_train.copy()\n",
    "y_train_balanced = pd.DataFrame()\n",
    "\n",
    "for label in y_train.columns:\n",
    "    # Check if the label has more than one class\n",
    "    if len(y_train[label].unique()) > 1:  # Skip SMOTE for single-class labels\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train[label])\n",
    "        X_train_balanced = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "        y_train_balanced[label] = y_resampled\n",
    "    else:\n",
    "        # Retain original values for single-class labels\n",
    "        y_train_balanced[label] = y_train[label]\n",
    "\n",
    "# Verify the new label distribution\n",
    "print(\"Balanced Training Set Label Distribution:\")\n",
    "print(y_train_balanced.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7e969b1-dd38-4a83-8ffe-480d9266f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Training Set Label Distribution:\n",
      "happiness    200.0\n",
      "sadness      176.0\n",
      "anger        122.0\n",
      "surprise       0.0\n",
      "fear         304.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Initialize placeholders for the balanced dataset\n",
    "X_train_balanced = X_train.copy()\n",
    "y_train_balanced = pd.DataFrame()\n",
    "\n",
    "# Apply SMOTE for each label independently\n",
    "for label in y_train.columns:\n",
    "    # Check if the label has more than one class\n",
    "    if len(y_train[label].unique()) > 1:  # Avoid errors for single-class labels\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train[label])\n",
    "        X_train_balanced = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "        y_train_balanced[label] = y_resampled\n",
    "    else:\n",
    "        # Retain original values for single-class labels\n",
    "        y_train_balanced[label] = y_train[label]\n",
    "\n",
    "# Verify the new label distribution\n",
    "print(\"Balanced Training Set Label Distribution:\")\n",
    "print(y_train_balanced.sum(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "317a0489-dbda-4693-bc11-c60950b5d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Dataset Shapes:\n",
      "X_train_augmented: (354, 6)\n",
      "y_train_augmented: (354, 5)\n",
      "\n",
      "Balanced Training Set Label Distribution:\n",
      "happiness    200.0\n",
      "sadness      274.0\n",
      "anger        328.0\n",
      "surprise     304.0\n",
      "fear         304.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Number of synthetic samples to create for 'surprise'\n",
    "n_synthetic_samples = 50\n",
    "\n",
    "# Generate random synthetic features for 'surprise'\n",
    "synthetic_features = pd.DataFrame(\n",
    "    np.random.rand(n_synthetic_samples, X_train.shape[1]),\n",
    "    columns=X_train.columns\n",
    ")\n",
    "\n",
    "# Add synthetic 'surprise' label (1) for these samples\n",
    "synthetic_labels = pd.DataFrame({\n",
    "    'happiness': [0] * n_synthetic_samples,\n",
    "    'sadness': [0] * n_synthetic_samples,\n",
    "    'anger': [0] * n_synthetic_samples,\n",
    "    'surprise': [1] * n_synthetic_samples,\n",
    "    'fear': [0] * n_synthetic_samples\n",
    "})\n",
    "\n",
    "# Append synthetic data to training set\n",
    "X_train_augmented = pd.concat([X_train, synthetic_features], ignore_index=True)\n",
    "y_train_augmented = pd.concat([y_train, synthetic_labels], ignore_index=True)\n",
    "\n",
    "# Verify the shapes are consistent\n",
    "print(\"Augmented Dataset Shapes:\")\n",
    "print(f\"X_train_augmented: {X_train_augmented.shape}\")\n",
    "print(f\"y_train_augmented: {y_train_augmented.shape}\")\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Placeholder for balanced datasets\n",
    "X_train_balanced = []\n",
    "y_train_balanced = []\n",
    "\n",
    "# Apply SMOTE for each label independently\n",
    "for label in y_train_augmented.columns:\n",
    "    if len(y_train_augmented[label].unique()) > 1:  # Apply SMOTE only if more than one class\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_augmented, y_train_augmented[label])\n",
    "        X_train_balanced.append(pd.DataFrame(X_resampled, columns=X_train.columns))\n",
    "        y_train_balanced.append(pd.DataFrame({label: y_resampled}))\n",
    "    else:\n",
    "        # Retain original values for single-class labels\n",
    "        X_train_balanced.append(X_train_augmented)\n",
    "        y_train_balanced.append(pd.DataFrame({label: y_train_augmented[label]}))\n",
    "\n",
    "# Concatenate results\n",
    "X_train_balanced = pd.concat(X_train_balanced).reset_index(drop=True)\n",
    "y_train_balanced = pd.concat(y_train_balanced, axis=1).reset_index(drop=True)\n",
    "\n",
    "# Verify the new label distribution\n",
    "print(\"\\nBalanced Training Set Label Distribution:\")\n",
    "print(y_train_balanced.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55e94ced-b386-4964-a1f4-27aa79b89e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values in y_train_balanced:\n",
      "happiness    256\n",
      "sadness      108\n",
      "anger          0\n",
      "surprise      48\n",
      "fear          48\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for NaN values in y_train_balanced:\")\n",
    "print(y_train_balanced.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5d45b6f-9e12-4dc4-a3da-4d4f317dd454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-cleaning check for NaN values:\n",
      "happiness    0\n",
      "sadness      0\n",
      "anger        0\n",
      "surprise     0\n",
      "fear         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN with 0 or appropriate default value\n",
    "y_train_balanced.fillna(0, inplace=True)\n",
    "\n",
    "# Verify no NaN values remain\n",
    "print(\"Post-cleaning check for NaN values:\")\n",
    "print(y_train_balanced.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b32c5a86-6f10-4aef-bff3-9883668cd581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_balanced: (2820, 6)\n",
      "Shape of y_train_balanced: (656, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_balanced:\", X_train_balanced.shape)\n",
    "print(\"Shape of y_train_balanced:\", y_train_balanced.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2116c9c-7a7d-4143-ad2d-ec597486224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found! Ensure alignment between features and labels.\n"
     ]
    }
   ],
   "source": [
    "if X_train_balanced.shape[0] != y_train_balanced.shape[0]:\n",
    "    print(\"Mismatch found! Ensure alignment between features and labels.\")\n",
    "else:\n",
    "    print(\"X_train_balanced and y_train_balanced are aligned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c37fffc-d6ad-4c76-963f-5efce39e5011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_augmented shape: (354, 5)\n",
      "y_train_augmented dtypes:\n",
      "happiness    int64\n",
      "sadness      int64\n",
      "anger        int64\n",
      "surprise     int64\n",
      "fear         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train_augmented shape:\", y_train_augmented.shape)\n",
    "print(\"y_train_augmented dtypes:\")\n",
    "print(y_train_augmented.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d3981e5-0273-4099-ab77-5309d6a27b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined label by avoiding index alignment issues\n",
    "binary_weights = 2 ** np.arange(len(y_train_augmented.columns))\n",
    "y_train_combined = y_train_augmented.dot(binary_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c81cfb6d-e126-4a17-b816-fbdde86baa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Labels (first 10):\n",
      "0    17\n",
      "1    17\n",
      "2    18\n",
      "3    17\n",
      "4    17\n",
      "5    18\n",
      "6    17\n",
      "7    16\n",
      "8    17\n",
      "9    17\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Labels (first 10):\")\n",
    "print(y_train_combined.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85b0a856-a004-4303-809d-acdc4e447f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote_combined = smote.fit_resample(X_train_augmented, y_train_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a94965af-740e-41b7-881d-4a794a647f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the decomposed labels\n",
    "y_train_smote = pd.DataFrame(index=X_train_smote.index, columns=y_train_augmented.columns)\n",
    "for i, col in enumerate(y_train_augmented.columns):\n",
    "    y_train_smote[col] = (y_train_smote_combined // (2**i)) % 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e9c2bfb-0ba9-40c7-913b-9743e7272855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shapes:\n",
      "X_train_smote: (864, 6)\n",
      "y_train_smote: (864, 5)\n",
      "Balanced Label Distribution:\n",
      "happiness    432\n",
      "sadness      288\n",
      "anger        144\n",
      "surprise     144\n",
      "fear         720\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced Dataset Shapes:\")\n",
    "print(\"X_train_smote:\", X_train_smote.shape)\n",
    "print(\"y_train_smote:\", y_train_smote.shape)\n",
    "\n",
    "print(\"Balanced Label Distribution:\")\n",
    "print(y_train_smote.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df0ae908-6af6-45ae-af7b-ed0ad6239506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model retraining completed!\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with the balanced dataset\n",
    "model_balanced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_balanced.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"Model retraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6101b3dd-313b-4dc8-844f-d43163c6b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.96      1.00      0.98        52\n",
      "     sadness       0.90      0.90      0.90        20\n",
      "       anger       1.00      1.00      1.00         2\n",
      "    surprise       0.00      0.00      0.00         0\n",
      "        fear       1.00      1.00      1.00        76\n",
      "\n",
      "   micro avg       0.97      0.99      0.98       150\n",
      "   macro avg       0.77      0.78      0.78       150\n",
      "weighted avg       0.97      0.99      0.98       150\n",
      " samples avg       0.98      0.99      0.98       150\n",
      "\n",
      "Hamming Loss: 0.0158\n",
      "Exact Match Ratio: 0.9211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reeva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\reeva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\reeva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, hamming_loss, accuracy_score\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = model_balanced.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_train_smote.columns))\n",
    "\n",
    "print(f\"Hamming Loss: {hamming_loss(y_test, y_pred):.4f}\")\n",
    "print(f\"Exact Match Ratio: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37a76372-fc75-47be-8634-beab8b0eab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance for happiness:\n",
      "prop_W: 0.2805584614116852\n",
      "prop_1: 0.14215132376955547\n",
      "prop_2: 0.19628159211078847\n",
      "prop_3: 0.05250339708654194\n",
      "prop_4: 0.12494752167796215\n",
      "prop_R: 0.20355770394346684\n",
      "Feature importance for sadness:\n",
      "prop_W: 0.2805584614116852\n",
      "prop_1: 0.14215132376955547\n",
      "prop_2: 0.19628159211078847\n",
      "prop_3: 0.05250339708654194\n",
      "prop_4: 0.12494752167796215\n",
      "prop_R: 0.20355770394346684\n",
      "Feature importance for anger:\n",
      "prop_W: 0.2805584614116852\n",
      "prop_1: 0.14215132376955547\n",
      "prop_2: 0.19628159211078847\n",
      "prop_3: 0.05250339708654194\n",
      "prop_4: 0.12494752167796215\n",
      "prop_R: 0.20355770394346684\n",
      "Feature importance for surprise:\n",
      "prop_W: 0.2805584614116852\n",
      "prop_1: 0.14215132376955547\n",
      "prop_2: 0.19628159211078847\n",
      "prop_3: 0.05250339708654194\n",
      "prop_4: 0.12494752167796215\n",
      "prop_R: 0.20355770394346684\n",
      "Feature importance for fear:\n",
      "prop_W: 0.2805584614116852\n",
      "prop_1: 0.14215132376955547\n",
      "prop_2: 0.19628159211078847\n",
      "prop_3: 0.05250339708654194\n",
      "prop_4: 0.12494752167796215\n",
      "prop_R: 0.20355770394346684\n"
     ]
    }
   ],
   "source": [
    "# Feature importance calculation\n",
    "for label in y_train.columns:\n",
    "    print(f\"Feature importance for {label}:\")\n",
    "    importances = model.feature_importances_\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        print(f\"{col}: {importances[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d644ffe-dafc-4278-b5c9-9c020d2e97d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance for happiness:\n",
      "[0.08621926 0.29064414 0.08284183 0.13191883 0.06020066 0.34817529]\n",
      "Feature importance for sadness:\n",
      "[0.20884938 0.09255665 0.16934973 0.10113268 0.06456743 0.36354414]\n",
      "Feature importance for anger:\n",
      "[0.3684925  0.076151   0.11795029 0.16170368 0.07146496 0.20423758]\n",
      "Feature importance for surprise:\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "Feature importance for fear:\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for label in y_train.columns:\n",
    "    model = RandomForestClassifier(random_state=42)  # New model instance\n",
    "    model.fit(X_train, y_train[label])\n",
    "    print(f\"Feature importance for {label}:\")\n",
    "    print(model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c69a2f5-2aaf-4328-b760-545e02fd3545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           prop_W      prop_1      prop_2      prop_3      prop_4      prop_R\n",
      "count  304.000000  304.000000  304.000000  304.000000  304.000000  304.000000\n",
      "mean     0.494150    0.261811    0.199627    0.190642    0.102738    0.195474\n",
      "std      0.209590    0.193246    0.165965    0.208045    0.188546    0.137298\n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000\n",
      "25%      0.517283    0.129933    0.099878    0.036994    0.000000    0.107579\n",
      "50%      0.594311    0.225976    0.137315    0.157200    0.013027    0.152900\n",
      "75%      0.621568    0.352060    0.207886    0.247826    0.125365    0.214980\n",
      "max      0.693147    1.000000    0.693147    1.000000    1.000000    0.693147\n",
      "happiness    200\n",
      "sadness       80\n",
      "anger         26\n",
      "surprise       0\n",
      "fear         304\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.describe())  # Check for variability\n",
    "print(y_train.sum(axis=0))  # Verify label distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8e163a0-12b7-4043-85a6-fbdb90901417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Balanced Training Set Label Distribution:\n",
      "happiness    200\n",
      "sadness      126\n",
      "anger         72\n",
      "surprise      96\n",
      "fear         304\n",
      "dtype: int64\n",
      "\n",
      "Corrected Dataset Shapes:\n",
      "X_train_balanced: (608, 6)\n",
      "y_train_balanced: (400, 5)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Initialize balanced dataset placeholders\n",
    "X_train_balanced = pd.DataFrame()\n",
    "y_train_balanced = pd.DataFrame()\n",
    "\n",
    "for label in y_train_augmented.columns:\n",
    "    # Check if the label has more than one class\n",
    "    if len(y_train_augmented[label].unique()) > 1:\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_augmented, y_train_augmented[label])\n",
    "\n",
    "        # Append only the newly generated rows using pd.concat\n",
    "        if X_train_balanced.empty:\n",
    "            X_train_balanced = pd.DataFrame(X_resampled, columns=X_train_augmented.columns)\n",
    "        else:\n",
    "            X_train_balanced = pd.concat([\n",
    "                X_train_balanced.iloc[:len(X_train_augmented)],\n",
    "                pd.DataFrame(X_resampled[len(X_train_augmented):], columns=X_train_augmented.columns)\n",
    "            ], ignore_index=True)\n",
    "\n",
    "        y_train_balanced[label] = pd.concat([\n",
    "            y_train_augmented[label],\n",
    "            pd.Series(y_resampled[len(y_train_augmented):])\n",
    "        ], ignore_index=True)\n",
    "    else:\n",
    "        # Retain original label values if SMOTE is not applied\n",
    "        if X_train_balanced.empty:\n",
    "            X_train_balanced = X_train_augmented.copy()\n",
    "        y_train_balanced[label] = y_train_augmented[label]\n",
    "\n",
    "# Verify the fixed dataset\n",
    "print(\"Corrected Balanced Training Set Label Distribution:\")\n",
    "print(y_train_balanced.sum(axis=0))\n",
    "print(\"\\nCorrected Dataset Shapes:\")\n",
    "print(\"X_train_balanced:\", X_train_balanced.shape)\n",
    "print(\"y_train_balanced:\", y_train_balanced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80a29900-eab7-4e13-b0f5-c6e64ab3931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Balanced Dataset Shapes:\n",
      "X_train_balanced: (354, 6)\n",
      "y_train_balanced: (354, 5)\n",
      "\n",
      "Balanced Label Distribution:\n",
      "happiness    200\n",
      "sadness       80\n",
      "anger         26\n",
      "surprise      50\n",
      "fear         304\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Initialize balanced datasets\n",
    "X_train_balanced = pd.DataFrame()\n",
    "y_train_balanced = pd.DataFrame()\n",
    "\n",
    "# Process each label independently\n",
    "for label in y_train_augmented.columns:\n",
    "    if len(y_train_augmented[label].unique()) > 1:  # Apply SMOTE if there are multiple classes\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_augmented, y_train_augmented[label])\n",
    "        new_samples = len(X_resampled) - len(X_train_augmented)\n",
    "\n",
    "        # Add resampled data\n",
    "        if X_train_balanced.empty:\n",
    "            X_train_balanced = X_train_augmented.copy()  # Start with original data\n",
    "        X_train_balanced = pd.concat(\n",
    "            [X_train_balanced, pd.DataFrame(X_resampled[-new_samples:], columns=X_train_augmented.columns)],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Update labels\n",
    "        if y_train_balanced.empty:\n",
    "            y_train_balanced = y_train_augmented.copy()  # Start with original data\n",
    "        y_train_balanced[label] = pd.concat(\n",
    "            [y_train_balanced[label], pd.Series(y_resampled[-new_samples:])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    else:\n",
    "        # Retain original data for single-class labels\n",
    "        if X_train_balanced.empty:\n",
    "            X_train_balanced = X_train_augmented\n",
    "        if y_train_balanced.empty:\n",
    "            y_train_balanced = y_train_augmented\n",
    "        y_train_balanced[label] = y_train_augmented[label]\n",
    "\n",
    "# Replace any NaN values with 0\n",
    "y_train_balanced = y_train_balanced.fillna(0)\n",
    "\n",
    "# Ensure alignment\n",
    "X_train_balanced = X_train_balanced.iloc[:y_train_balanced.shape[0]]\n",
    "\n",
    "# Verify the results\n",
    "assert X_train_balanced.shape[0] == y_train_balanced.shape[0], (\n",
    "    f\"Mismatch in samples: X_train_balanced ({X_train_balanced.shape[0]}), y_train_balanced ({y_train_balanced.shape[0]})\"\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(\"Final Balanced Dataset Shapes:\")\n",
    "print(f\"X_train_balanced: {X_train_balanced.shape}\")\n",
    "print(f\"y_train_balanced: {y_train_balanced.shape}\")\n",
    "\n",
    "print(\"\\nBalanced Label Distribution:\")\n",
    "print(y_train_balanced.sum(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b8be031-e65d-433f-823e-4ffbec77ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Means After Normalization (Should Be ~0):\n",
      " prop_W    4.817239e-16\n",
      "prop_1   -2.007183e-17\n",
      "prop_2   -8.028731e-17\n",
      "prop_3    1.405028e-16\n",
      "prop_4   -2.007183e-17\n",
      "prop_R    1.605746e-16\n",
      "dtype: float64\n",
      "\n",
      "Feature Standard Deviations After Normalization (Should Be ~1):\n",
      " prop_W    1.001415\n",
      "prop_1    1.001415\n",
      "prop_2    1.001415\n",
      "prop_3    1.001415\n",
      "prop_4    1.001415\n",
      "prop_R    1.001415\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling only to features\n",
    "X_train_normalized = scaler.fit_transform(X_train_balanced)\n",
    "\n",
    "# Convert back to DataFrame for better readability\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized, columns=X_train_balanced.columns)\n",
    "\n",
    "# Verify the normalized data\n",
    "print(\"Feature Means After Normalization (Should Be ~0):\\n\", X_train_normalized.mean())\n",
    "print(\"\\nFeature Standard Deviations After Normalization (Should Be ~1):\\n\", X_train_normalized.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3384c6b2-59de-4e86-a00a-e4ea149a3ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Training completed!\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.98      1.00      0.99        41\n",
      "     sadness       1.00      0.92      0.96        13\n",
      "       anger       1.00      1.00      1.00         6\n",
      "    surprise       1.00      0.90      0.95        10\n",
      "        fear       0.98      1.00      0.99        61\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       131\n",
      "   macro avg       0.99      0.96      0.98       131\n",
      "weighted avg       0.99      0.98      0.98       131\n",
      " samples avg       0.99      0.98      0.98       131\n",
      "\n",
      "Hamming Loss: 0.0113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, hamming_loss\n",
    "\n",
    "# Step 1: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_normalized, y_train_balanced, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Step 3: Train the model\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_train.columns))\n",
    "\n",
    "# Compute the Hamming Loss\n",
    "hamming_loss_value = hamming_loss(y_test, y_pred)\n",
    "print(f\"Hamming Loss: {hamming_loss_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d29216c-d333-434c-8848-55da75af0069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1-scores: [0.95023474 1.         0.96807512 0.95492958 0.28285714]\n",
      "Mean CV F1-score: 0.8312193158953722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train_normalized, y_train_balanced, cv=5, scoring='f1_samples')\n",
    "print(\"Cross-Validation F1-scores:\", scores)\n",
    "print(\"Mean CV F1-score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5a81da3-d970-4e30-9dd4-92073a3ca896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "  Feature  Importance\n",
      "0  prop_W    0.222886\n",
      "5  prop_R    0.210152\n",
      "1  prop_1    0.167590\n",
      "4  prop_4    0.164978\n",
      "2  prop_2    0.152173\n",
      "3  prop_3    0.082220\n"
     ]
    }
   ],
   "source": [
    "# Extract feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a5d8327-ebc9-4eb8-b0b6-c34b7414714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance for happiness:\n",
      "  prop_W: 0.2878\n",
      "  prop_1: 0.2270\n",
      "  prop_2: 0.1019\n",
      "  prop_3: 0.0388\n",
      "  prop_4: 0.1790\n",
      "  prop_R: 0.1655\n",
      "Feature importance for sadness:\n",
      "  prop_W: 0.2259\n",
      "  prop_1: 0.0951\n",
      "  prop_2: 0.3061\n",
      "  prop_3: 0.0680\n",
      "  prop_4: 0.1440\n",
      "  prop_R: 0.1609\n",
      "Feature importance for anger:\n",
      "  prop_W: 0.2381\n",
      "  prop_1: 0.1114\n",
      "  prop_2: 0.0401\n",
      "  prop_3: 0.0962\n",
      "  prop_4: 0.2955\n",
      "  prop_R: 0.2188\n",
      "Feature importance for surprise:\n",
      "  prop_W: 0.2292\n",
      "  prop_1: 0.0814\n",
      "  prop_2: 0.1550\n",
      "  prop_3: 0.0797\n",
      "  prop_4: 0.1624\n",
      "  prop_R: 0.2922\n",
      "Feature importance for fear:\n",
      "  prop_W: 0.1428\n",
      "  prop_1: 0.2630\n",
      "  prop_2: 0.1017\n",
      "  prop_3: 0.1514\n",
      "  prop_4: 0.0727\n",
      "  prop_R: 0.2685\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each label and calculate feature importance\n",
    "for label_idx, label in enumerate(y_train.columns):\n",
    "    print(f\"Feature importance for {label}:\")\n",
    "    # Access the individual estimator for the current label\n",
    "    label_model = model.estimators_[label_idx]\n",
    "    importances = label_model.feature_importances_\n",
    "\n",
    "    # Print feature importance for each feature\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        print(f\"  {col}: {importances[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78496938-517f-41ca-98b8-90da26ab897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1-score across folds: 0.8504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train_normalized, y_train_balanced, cv=5, scoring='f1_micro')\n",
    "print(f\"Mean F1-score across folds: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a77c3eff-93e6-46af-8e22-2604c82466f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test dtypes:\n",
      "happiness    int64\n",
      "sadness      int64\n",
      "anger        int64\n",
      "surprise     int64\n",
      "fear         int64\n",
      "dtype: object\n",
      "\n",
      "y_pred dtypes:\n",
      "happiness    int64\n",
      "sadness      int64\n",
      "anger        int64\n",
      "surprise     int64\n",
      "fear         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"y_test dtypes:\")\n",
    "print(y_test.dtypes)\n",
    "\n",
    "print(\"\\ny_pred dtypes:\")\n",
    "print(pd.DataFrame(y_pred, columns=y_train.columns).dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e225397a-c65c-4c1a-9117-4b03cb13282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (71, 5)\n",
      "y_pred shape: (71, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"y_pred shape: {y_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb8f83fd-ae9f-4ac3-b6d5-08d21f92e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test:\n",
      "   happiness  sadness  anger  surprise  fear\n",
      "0          1        0      0         0     1\n",
      "1          0        1      1         1     0\n",
      "Unique values in y_pred:\n",
      "   happiness  sadness  anger  surprise  fear\n",
      "0          1        0      0         0     1\n",
      "1          0        1      1         1     0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique values in y_test:\\n{y_test.apply(pd.Series.unique)}\")\n",
    "print(f\"Unique values in y_pred:\\n{pd.DataFrame(y_pred, columns=y_train.columns).apply(pd.Series.unique)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48d927cc-39c0-4920-9481-7f109fea1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for happiness:\n",
      "[[29  1]\n",
      " [ 0 41]]\n",
      "Confusion Matrix for sadness:\n",
      "[[58  0]\n",
      " [ 1 12]]\n",
      "Confusion Matrix for anger:\n",
      "[[65  0]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for surprise:\n",
      "[[61  0]\n",
      " [ 1  9]]\n",
      "Confusion Matrix for fear:\n",
      "[[ 9  1]\n",
      " [ 0 61]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Ensure y_pred is in the correct format (DataFrame for column alignment)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "\n",
    "confusion_matrices = multilabel_confusion_matrix(y_test, y_pred_df)\n",
    "for idx, label in enumerate(y_train.columns):\n",
    "    print(f\"Confusion Matrix for {label}:\\n{confusion_matrices[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21bf85e9-ecca-4f87-8786-ca7187df9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Google Drive at C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/multi_label_model.joblib\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "import joblib\n",
    "\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Define path in Google Drive\n",
    "model_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/multi_label_model.joblib'\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to Google Drive at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38ee4372-ef7b-4a73-87c9-1a9a816a08da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion prediction for the random record at index 97:\n",
      "           Predicted\n",
      "happiness          1\n",
      "sadness            0\n",
      "anger              0\n",
      "surprise           0\n",
      "fear               1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reeva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/multi_label_model.joblib'\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Load the test dataset\n",
    "# Ensure that you have the same normalization and feature set applied as during training\n",
    "test_data_path = 'C:/Users/reeva/Desktop/690/sleep-edf-database-expanded-1.0.0-20241127T213628Z-001/sleep-edf-database-expanded-1.0.0/aggregated_sleep_data.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Select relevant features for prediction (same as training features)\n",
    "feature_columns = ['prop_W', 'prop_1', 'prop_2', 'prop_3', 'prop_4', 'prop_R']\n",
    "X_test = test_data[feature_columns]\n",
    "\n",
    "# Apply the same scaler as during the training\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)  # Fit to entire test data for demonstration purposes\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Pick one random record from the test data\n",
    "random_index = random.randint(0, len(X_test_normalized) - 1)\n",
    "random_sample = X_test_normalized[random_index].reshape(1, -1)\n",
    "\n",
    "\n",
    "# Predict the emotion for this record\n",
    "y_pred_random = model.predict(random_sample)\n",
    "\n",
    "# If it's a multi-label model, ensure the output is binary (0/1) for all classes\n",
    "y_pred_random_df = pd.DataFrame([y_pred_random[0]], columns=emotion_labels)\n",
    "\n",
    "# Print the prediction result\n",
    "print(f\"Emotion prediction for the random record at index {random_index}:\")\n",
    "print(y_pred_random_df.T.rename(columns={0: 'Predicted'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbc01b9c-c271-471d-913f-3e1dde2b1881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion prediction for the random record (as percentages):\n",
      "happiness: 50.00%\n",
      "fear: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# If no emotion is predicted, set all to 0%\n",
    "predicted_emotions = y_pred_random\n",
    "if np.sum(predicted_emotions) == 0:\n",
    "    emotion_percentages = [0 for _ in predicted_emotions[0]]\n",
    "else:\n",
    "    # Normalize each active emotion to sum to 100%\n",
    "    total_active = np.sum(predicted_emotions[0])\n",
    "    emotion_percentages = [(value / total_active) * 100 if value == 1 else 0 for value in predicted_emotions[0]]\n",
    "\n",
    "# Prepare the output\n",
    "emotion_percentages_df = pd.DataFrame({\n",
    "    'Emotion': emotion_labels,\n",
    "    'Percentage': emotion_percentages\n",
    "})\n",
    "\n",
    "# Print the prediction result as percentages\n",
    "print(\"Emotion prediction for the random record (as percentages):\")\n",
    "for index, row in emotion_percentages_df.iterrows():\n",
    "    if row['Percentage'] > 0:  # Only print emotions with non-zero percentages\n",
    "        print(f\"{row['Emotion']}: {row['Percentage']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f2a7e-e24e-4747-a555-12961b7fdfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
