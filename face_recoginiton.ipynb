{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4e0074-fefe-4660-b00b-f38c398ff975",
   "metadata": {},
   "source": [
    "# Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31119a25-8a7c-4db9-9160-ff1eda3bea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports loaded successfully.\n",
      "Paths set: \n",
      "Train Directory: C:/Users/reeva/Desktop/690/fer-2013\\train \n",
      "Test Directory: C:/Users/reeva/Desktop/690/fer-2013\\test\n",
      "Loading saved model from best_model.keras...\n",
      "Model loaded successfully.\n",
      "Face classifier loaded successfully.\n",
      "Webcam accessed successfully. Press 'c' to capture a photo.\n",
      "Photo captured. Analyzing...\n",
      "Angry: 91.76%\n",
      "Disgust: 1.88%\n",
      "Fear: 5.89%\n",
      "Happy: 0.06%\n",
      "Sad: 0.02%\n",
      "Surprise: 0.26%\n",
      "Neutral: 0.13%\n",
      "Emotion detection completed successfully.\n",
      "Webcam feed closed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "print(\"All imports loaded successfully.\")\n",
    "\n",
    "# Cell 2: Set paths to train and test directories\n",
    "train_directory = os.path.join('C:/Users/reeva/Desktop/690/fer-2013', 'train')\n",
    "test_directory = os.path.join('C:/Users/reeva/Desktop/690/fer-2013', 'test')\n",
    "model_path = 'best_model.keras'  # Path to save/load the trained model\n",
    "\n",
    "print(\"Paths set: \\nTrain Directory:\", train_directory, \"\\nTest Directory:\", test_directory)\n",
    "\n",
    "# Cell 3: Check if the model exists\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading saved model from {model_path}...\")\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    # Cell 4: Create ImageDataGenerators for training and testing with more augmentation\n",
    "    try:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1.0/255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            brightness_range=[0.8, 1.2]\n",
    "        )\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "        print(\"ImageDataGenerators created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in creating ImageDataGenerators:\", e)\n",
    "\n",
    "    # Cell 5: Set batch size and create train/test generators\n",
    "    batch_size = 32\n",
    "\n",
    "    try:\n",
    "        # Use RGB to match VGG16 input expectations\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_directory,\n",
    "            target_size=(48, 48),\n",
    "            color_mode='rgb',\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(\"Training generator created successfully.\")\n",
    "        print(f\"Number of training samples: {train_generator.samples}\")\n",
    "        print(f\"Number of classes (labels): {train_generator.num_classes}\")\n",
    "        print(f\"Class labels: {train_generator.class_indices}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in creating training generator:\", e)\n",
    "\n",
    "    try:\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_directory,\n",
    "            target_size=(48, 48),\n",
    "            color_mode='rgb',\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical'\n",
    "        )\n",
    "        print(\"Test generator created successfully.\")\n",
    "        print(f\"Number of test samples: {test_generator.samples}\")\n",
    "        print(f\"Number of classes (labels): {test_generator.num_classes}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in creating test generator:\", e)\n",
    "\n",
    "    # Cell 6: Fine-tune some VGG16 layers and define the new model\n",
    "    try:\n",
    "        print(\"Loading VGG16 base model...\")\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "        # Unfreeze the last few convolutional layers to allow fine-tuning\n",
    "        for layer in base_model.layers[-4:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "        print(\"VGG16 base model loaded and new model defined successfully.\")\n",
    "        \n",
    "        # Compile the model with Adam optimizer\n",
    "        print(\"Compiling the model...\")\n",
    "        model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(\"Model compiled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in defining/compiling the model:\", e)\n",
    "\n",
    "    # Cell 7: Set up callbacks for saving the best model, early stopping, and reducing the learning rate on plateau\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    # Cell 8: Fit the model without class weights\n",
    "    try:\n",
    "        print(\"Starting model training without class weights...\")\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=test_generator,\n",
    "            epochs=50,\n",
    "            steps_per_epoch=train_generator.samples // batch_size,\n",
    "            validation_steps=test_generator.samples // batch_size,\n",
    "            callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "            verbose=1  # Set verbosity to see training progress\n",
    "        )\n",
    "        best_accuracy = max(history.history['val_accuracy'])\n",
    "        print(f\"Model training completed with highest validation accuracy: {best_accuracy * 100:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during model training:\", e)\n",
    "\n",
    "    # The model is saved using ModelCheckpoint callback\n",
    "    print(f\"Training complete. Model saved as '{model_path}'.\")\n",
    "\n",
    "# Cell 9: Load the face classifier (for real-time inference later)\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "print(\"Face classifier loaded successfully.\")\n",
    "\n",
    "# Cell 10: Define a decorated function for prediction to avoid retracing\n",
    "@tf.function\n",
    "def predict_emotion(roi):\n",
    "    return model(roi, training=False)\n",
    "\n",
    "# Cell 11: Define function for detecting and predicting emotions\n",
    "def detect_emotion(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        if np.sum([roi_gray]) != 0:\n",
    "            # Convert grayscale image to RGB by stacking the same channel 3 times\n",
    "            roi_rgb = np.stack((roi_gray,) * 3, axis=-1)\n",
    "            roi = roi_rgb.astype('float32') / 255.0\n",
    "            roi = np.expand_dims(roi, axis=0)  # Add batch dimension\n",
    "\n",
    "            prediction = predict_emotion(roi)[0].numpy()  # Use decorated function\n",
    "            emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "            emotion_results = {emotions[idx]: prediction[idx] * 100 for idx in range(len(emotions))}\n",
    "            \n",
    "            # Print emotions and their confidence percentages\n",
    "            for emotion, confidence in emotion_results.items():\n",
    "                print(f\"{emotion}: {confidence:.2f}%\")\n",
    "                \n",
    "            return emotion_results\n",
    "\n",
    "    return None\n",
    "\n",
    "# Cell 12: Capture image from webcam and detect emotion\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "else:\n",
    "    print(\"Webcam accessed successfully. Press 'c' to capture a photo.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Emotion Detector - Press \"c\" to capture', frame)\n",
    "\n",
    "    # Press 'c' to capture the photo and analyze it\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "        print(\"Photo captured. Analyzing...\")\n",
    "        emotion_results = detect_emotion(frame)\n",
    "        if emotion_results:\n",
    "            print(\"Emotion detection completed successfully.\")\n",
    "        else:\n",
    "            print(\"No face detected. Please try again.\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Webcam feed closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d5252-3178-45be-9af6-56c86b0b2968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
